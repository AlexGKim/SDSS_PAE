{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f65200-2db4-4ddb-9069-06f59a58b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1a59e5-b64c-4e4c-a96b-8ff0e6d7d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5d9ec3-a68a-40fa-94c2-01e1e527c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45160da5-5cea-475f-8aeb-15f2f93352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85aa940-18bb-456c-addc-ddca10c54be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e213fc09-1720-4947-873b-ec15597ee0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ace569e-a952-48dd-b633-699fbb3c27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computes output shape for both convolutions and pooling layers\n",
    "def output_shape(in_dim,stride,padding,kernel,dilation=1):\n",
    "    out_dim = np.floor((in_dim + 2*padding - dilation*(kernel-1)-1)/stride+1).astype(int)\n",
    "    return out_dim\n",
    "\n",
    "def output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation=1):\n",
    "    out_dim = (in_dim-1)*stride-2*padding+dilation*(kernel-1)+output_padding+1\n",
    "    return out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa54417-1d5d-4ce7-9081-6359c4c8c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape(np.arange(5),stride=1,padding=0,kernel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c5ce73-6c2c-413c-a517-261aea0091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dilation(out_dim,in_dim,stride,padding,kernel,output_padding):\n",
    "    dilation = np.floor((out_dim-(in_dim-1)*stride+2*padding-output_padding-1)/(kernel-1))\n",
    "    new_dim  = output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation)\n",
    "    if new_dim == out_dim:\n",
    "        pass\n",
    "    else:\n",
    "        output_padding = (out_dim-new_dim)\n",
    "    return dilation, output_padding\n",
    "\n",
    "def get_output_padding(in_dim,out_dim, stride,padding,kernel,dilation=1):\n",
    "    return out_dim-(in_dim-1)*stride+2*padding-dilation*(kernel-1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21fa279-be16-453d-b901-f76ed2774cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a878e81-e0a4-454a-80d5-0e80c0220cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCEncoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(FCEncoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        \n",
    "        current_dim = params['input_dim']**self.N*params['input_c']\n",
    "        \n",
    "        for ii in range(nparams['n_layers']):\n",
    "            \n",
    "            lin = nn.Linear(current_dim, nparams['out_sizes'][ii])\n",
    "            self.model.append(spec_norm(lin))\n",
    "            \n",
    "            current_dim      =  nparams['out_sizes'][ii]\n",
    "            \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm(current_dim,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)\n",
    "            \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            dropout = nn.Dropout(nparams['dropout_rate'][ii])\n",
    "            self.model.append(dropout)\n",
    "        \n",
    "        lin = nn.Linear(current_dim,params['latent_dim'])\n",
    "        self.model.append(spec_norm(lin))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099a6bc8-dc0e-4e46-957c-39e20f75fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.AdaptiveMaxPool1d#nn.MaxPool1d\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.AdaptiveMaxPool2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        current_channels   = params['input_c']\n",
    "        current_dim        = params['input_dim']\n",
    "        self.out_dims      = []\n",
    "        \n",
    "        for ii in range(nparams['n_layers']):\n",
    "            \n",
    "            conv = self.conv(current_channels, nparams['out_channels'][ii], nparams['kernel_sizes'][ii], nparams['strides'][ii], nparams['paddings'][ii])\n",
    "            self.out_dims.append(current_dim)\n",
    "            self.model.append(spec_norm(conv))\n",
    "            \n",
    "            current_channels =  nparams['out_channels'][ii]\n",
    "            current_dim      =  output_shape(current_dim, nparams['strides'][ii], nparams['paddings'][ii],nparams['kernel_sizes'][ii])\n",
    "            \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm([current_channels]+[current_dim]*self.N,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)\n",
    "                \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            pool = self.pool([current_dim//nparams['scale_facs'][ii]]*self.N)\n",
    "            self.model.append(pool)\n",
    "            \n",
    "            current_dim = current_dim//nparams['scale_facs'][ii]\n",
    "\n",
    "        self.final_dim = current_dim\n",
    "        self.final_c   = current_channels\n",
    "        \n",
    "        self.model.append(nn.Flatten())\n",
    "        current_shape = current_channels*current_dim**self.N\n",
    "        linear        = nn.Linear(current_shape,params['latent_dim'])\n",
    "        self.model.append(spec_norm(linear))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "\n",
    "            x = l(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8773f4e-9331-4073-ba73-beb8a765c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        if params['dim'] == '1D':\n",
    "            self.conv = nn.ConvTranspose1d\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.conv = nn.ConvTranspose2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "        \n",
    "        self.pool   = nn.Upsample\n",
    "        \n",
    "        self.model  = nn.ModuleList()\n",
    "        \n",
    "        final_shape = nparams['final_c']*nparams['final_dim']**self.N\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        lin         = nn.Linear(params['latent_dim'],final_shape)\n",
    "        self.model.append(spec_norm(lin))\n",
    "\n",
    "        if params['dim'] == '1D':\n",
    "            self.model.append(Reshape((-1, nparams['final_c'],nparams['final_dim'])))\n",
    "        else:\n",
    "            self.model.append(Reshape((-1, nparams['final_c'],nparams['final_dim'],nparams['final_dim'])))\n",
    "                              \n",
    "        current_dim      = nparams['final_dim']\n",
    "        current_channels = nparams['final_c']\n",
    "            \n",
    "        for jj in range(1,nparams['n_layers']+1):\n",
    "            ii = nparams['n_layers'] - jj \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "                  \n",
    "            upsample    = nn.Upsample(scale_factor=nparams['scale_facs'][ii])\n",
    "            self.model.append(upsample)\n",
    "            current_dim = current_dim*nparams['scale_facs'][ii]\n",
    "                              \n",
    "            output_padding = get_output_padding(current_dim,nparams['out_dims'][ii],nparams['strides'][ii],nparams['paddings'][ii],nparams['kernel_sizes'][ii],dilation=1)\n",
    "            conv           = self.conv(current_channels, nparams['out_channels'][ii], kernel_size=nparams['kernel_sizes'][ii], stride=nparams['strides'][ii], padding=nparams['paddings'][ii], output_padding=output_padding)\n",
    "            self.model.append(spec_norm(conv))\n",
    "            \n",
    "            current_channels = nparams['out_channels'][ii]\n",
    "            current_dim      = output_shape_transpose(current_dim, stride=nparams['strides'][ii], padding=nparams['paddings'][ii],kernel=nparams['kernel_sizes'][ii],output_padding=output_padding)\n",
    "                \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm([current_channels]+[current_dim]*self.N,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)    \n",
    "                \n",
    "        \n",
    "        conv = self.conv(current_channels, 1, kernel_size=1, stride=1)\n",
    "        self.model.append(spec_norm(conv))\n",
    "        \n",
    "        if nparams['image_data']: \n",
    "            self.model.append(getattr(nn, 'Sigmoid')())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1326674-3b3b-4775-9a54-7de19ac99cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDecoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(FCDecoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        \n",
    "        current_dim = params['latent_dim']\n",
    "        \n",
    "        for jj in range(1,nparams['n_layers']+1):\n",
    "            ii = nparams['n_layers'] - jj \n",
    "            \n",
    "            lin = nn.Linear(current_dim, nparams['out_sizes'][ii])\n",
    "            self.model.append(spec_norm(lin))\n",
    "            \n",
    "            current_dim      =  nparams['out_sizes'][ii]\n",
    "            \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm(current_dim,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)\n",
    "                \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            dropout = nn.Dropout(nparams['dropout_rate'][ii])\n",
    "            self.model.append(dropout)\n",
    "        \n",
    "        lin = nn.Linear(current_dim,params['input_dim']**self.N*params['input_c'])\n",
    "        self.model.append(spec_norm(lin))\n",
    "        \n",
    "        if nparams['image_data']: \n",
    "            self.model.append(getattr(nn, 'Sigmoid')())\n",
    "        \n",
    "        self.model.append(Reshape([-1]+[params['input_c']]+[params['input_dim']]*self.N))\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b7aae5-9771-4a32-99f7-823bc046a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, loc, batchsize):\n",
    "    \n",
    "    if data in dir(datasets):\n",
    "        dataset = getattr(datasets,data)\n",
    "    \n",
    "        training_data = dataset(root=loc,train=True,download=True,transform=ToTensor())\n",
    "\n",
    "        valid_data    = dataset(root=loc,train=False,download=True,transform=ToTensor())\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    train_dataloader = DataLoader(training_data, batch_size=batchsize, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size=256, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bcb7620-3c76-4891-80f9-61b62d31c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, params, dparams, nparams_enc, nparams_dec, tparams):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        if params['encoder_type'] == 'conv':\n",
    "            self.encoder = ConvEncoder(params, nparams_enc)\n",
    "            nparams_enc['out_dims']  = self.encoder.out_dims\n",
    "            nparams_enc['final_dim'] = self.encoder.final_dim\n",
    "            nparams_enc['final_c']   = self.encoder.final_c\n",
    "        elif params['encoder_type'] == 'fc':\n",
    "            self.encoder = FCEncoder(params, nparams_enc)\n",
    "        else:\n",
    "            raise Exception('invalid encoder type')\n",
    "            \n",
    "        if params['decoder_type'] == 'conv':\n",
    "            self.decoder = ConvDecoder(params, nparams_dec)\n",
    "        elif params['decoder_type'] == 'fc':\n",
    "            self.decoder = FCDecoder(params, n_dec)\n",
    "        else:\n",
    "            raise Exception('invalid decoder type')\n",
    "        \n",
    "        self.optimizer = getattr(optim, tparams['optimizer'])\n",
    "        self.optimizer = self.optimizer(self.parameters(),tparams['initial_lr'])\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, tparams['scheduler']),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**tparams['scheduler_params'])\n",
    "        \n",
    "        self.criterion = getattr(nn, tparams['criterion'])()\n",
    "        \n",
    "        self.train_loader, valid_loader = get_data(dparams['dataset'],dparams['loc'],tparams['batchsize'])\n",
    "        \n",
    "        self.valid_loader = iter(valid_loader)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def update_lr(self,lr):\n",
    "    \n",
    "        self.optimizer = getattr(optim, tparams['optimizer'])\n",
    "        self.optimizer  = self.optimizer(self.parameters(),lr)\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, tparams['scheduler']),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**tparams['scheduler_params'])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def update_scheduler(self,scheduler, scheduler_params):\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, scheduler),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**scheduler_params)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def update_optimizer(self,optimizer):\n",
    "        \n",
    "        self.optimizer = getattr(optim, optimizer)\n",
    "        self.optimizer  = self.optimizer(self.parameters(),tparams['initial_lr'])\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, tparams['scheduler']),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**tparams['scheduler_params'])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def train(self, nepochs):\n",
    "        running_loss    = []\n",
    "        validation_loss = []\n",
    "        for epoch in range(nepochs):\n",
    "            r_loss = 0\n",
    "            for ii, (data, _) in enumerate(self.train_loader,0):\n",
    "                data = data.to(device)\n",
    "                recon = self.forward(data)\n",
    "                loss  = self.criterion(recon, data)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                r_loss+=loss.item()\n",
    "            self.scheduler.step()\n",
    "            running_loss.append(r_loss/ii)\n",
    "            valid_data, _ = next(self.valid_loader)\n",
    "            valid_data = valid_data.to(device)\n",
    "            recon      = self.forward(valid_data)\n",
    "            loss       = self.criterion(recon, valid_data)\n",
    "            validation_loss.append(loss)\n",
    "            print(f'epoch: {epoch:d}, training loss: {running_loss[-1]:.4e}, validation loss: {loss:.4e}, learning rate: {self.scheduler.get_last_lr()[0]:.4e}')\n",
    "        return running_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3cb2599a-f4ab-4cfd-b54e-6f51cdadfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers     = 3\n",
    "out_channels = [16,16,16]\n",
    "kernel_sizes = [4,3,2]\n",
    "### stride,padding,kernel; [1,0,1] is identity\n",
    "#pooling_layers = [[1,0,1], [1,0,2]]\n",
    "scale_facs   = [1,1,1] \n",
    "paddings     = [0,0,0]\n",
    "strides      = [2,1,1]\n",
    "layer_norm   = [True,True,True]\n",
    "dropout_rate = [0.,0.,0.]\n",
    "spec_norm    = True\n",
    "dim          = '2D'\n",
    "activations  = ['ReLU', 'ReLU','ReLU']\n",
    "latent_dim   = 8\n",
    "input_c      = 1 \n",
    "input_dim    = 28\n",
    "encoder_type = 'conv'\n",
    "decoder_type = 'conv'\n",
    "affine       = False\n",
    "out_sizes    = [256,128,64]\n",
    "image_data   = True\n",
    "\n",
    "nepochs       = 20\n",
    "batchsize     = 64\n",
    "initial_lr    = 1e-2\n",
    "\n",
    "optimizer     = 'Adam'\n",
    "criterion     = 'MSELoss'\n",
    "\n",
    "scheduler     = 'ExponentialLR'\n",
    "scheduler_params = {'gamma':0.95}\n",
    "\n",
    "dataset       = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45c28651-e623-4d21-9038-12ece17fbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params      = {'input_c': input_c, 'input_dim': input_dim, 'latent_dim': latent_dim, 'encoder_type': encoder_type, 'decoder_type': decoder_type, 'dim': dim}\n",
    "conv_network_params = {'n_layers': n_layers, 'out_channels': out_channels, 'kernel_sizes': kernel_sizes, 'scale_facs': scale_facs, 'paddings': paddings,\\\n",
    "                       'strides': strides,'activations': activations, 'spec_norm': spec_norm, 'dropout_rate':dropout_rate, 'layer_norm': layer_norm,\\\n",
    "                       'affine': affine,'image_data': image_data}\n",
    "fc_network_params   = {'n_layers': n_layers, 'out_sizes': out_sizes,'activations': activations, 'spec_norm': spec_norm, 'dropout_rate':dropout_rate, \\\n",
    "                       'layer_norm': layer_norm, 'affine': affine, 'image_data': image_data}\n",
    "\n",
    "training_params     = {'batchsize': batchsize, 'initial_lr': initial_lr, 'optimizer': optimizer, 'criterion': criterion, \\\n",
    "                       'scheduler': scheduler, 'scheduler_params':scheduler_params}\n",
    "data_params         = {'dataset':dataset, 'loc': '/global/cscratch1/sd/vboehm/Datasets'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b1a95980-c3a4-4de6-901f-ce91aae482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = Autoencoder(general_params,data_params,conv_network_params, conv_network_params, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1af0f28d-bc4f-44f3-999d-97a04331095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(AE.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9604fb11-3562-4b6f-9bfd-c14bf8ef035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8d7ec4ed-be9b-43c6-b1d4-e483e534190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): ConvEncoder(\n",
       "    (model): ModuleList(\n",
       "      (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): LayerNorm((16, 13, 13), eps=1e-05, elementwise_affine=False)\n",
       "      (2): ReLU()\n",
       "      (3): AdaptiveMaxPool2d(output_size=[13, 13])\n",
       "      (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): LayerNorm((16, 11, 11), eps=1e-05, elementwise_affine=False)\n",
       "      (6): ReLU()\n",
       "      (7): AdaptiveMaxPool2d(output_size=[11, 11])\n",
       "      (8): Conv2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (9): LayerNorm((16, 10, 10), eps=1e-05, elementwise_affine=False)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveMaxPool2d(output_size=[10, 10])\n",
       "      (12): Flatten(start_dim=1, end_dim=-1)\n",
       "      (13): Linear(in_features=1600, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvDecoder(\n",
       "    (model): ModuleList(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=8, out_features=1600, bias=True)\n",
       "      (2): Reshape()\n",
       "      (3): ReLU()\n",
       "      (4): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (5): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (6): LayerNorm((16, 11, 11), eps=1e-05, elementwise_affine=False)\n",
       "      (7): ReLU()\n",
       "      (8): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (9): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (10): LayerNorm((16, 13, 13), eps=1e-05, elementwise_affine=False)\n",
       "      (11): ReLU()\n",
       "      (12): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (13): ConvTranspose2d(16, 16, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (14): LayerNorm((16, 28, 28), eps=1e-05, elementwise_affine=False)\n",
       "      (15): ConvTranspose2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (16): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5298e3f4-45de-49d6-b096-2c701b05a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  torch.randn(1,1,28,28).to('cuda')\n",
    "\n",
    "\n",
    "res = AE.forward(train_features.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9175fd50-35ec-416a-8f8e-21ff06de672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 13, 13]             272\n",
      "         LayerNorm-2           [-1, 16, 13, 13]               0\n",
      "              ReLU-3           [-1, 16, 13, 13]               0\n",
      " AdaptiveMaxPool2d-4           [-1, 16, 13, 13]               0\n",
      "            Conv2d-5           [-1, 16, 11, 11]           2,320\n",
      "         LayerNorm-6           [-1, 16, 11, 11]               0\n",
      "              ReLU-7           [-1, 16, 11, 11]               0\n",
      " AdaptiveMaxPool2d-8           [-1, 16, 11, 11]               0\n",
      "            Conv2d-9           [-1, 16, 10, 10]           1,040\n",
      "        LayerNorm-10           [-1, 16, 10, 10]               0\n",
      "             ReLU-11           [-1, 16, 10, 10]               0\n",
      "AdaptiveMaxPool2d-12           [-1, 16, 10, 10]               0\n",
      "          Flatten-13                 [-1, 1600]               0\n",
      "           Linear-14                    [-1, 8]          12,808\n",
      "      ConvEncoder-15                    [-1, 8]               0\n",
      "          Flatten-16                    [-1, 8]               0\n",
      "           Linear-17                 [-1, 1600]          14,400\n",
      "          Reshape-18           [-1, 16, 10, 10]               0\n",
      "             ReLU-19           [-1, 16, 10, 10]               0\n",
      "         Upsample-20           [-1, 16, 10, 10]               0\n",
      "  ConvTranspose2d-21           [-1, 16, 11, 11]           1,040\n",
      "        LayerNorm-22           [-1, 16, 11, 11]               0\n",
      "             ReLU-23           [-1, 16, 11, 11]               0\n",
      "         Upsample-24           [-1, 16, 11, 11]               0\n",
      "  ConvTranspose2d-25           [-1, 16, 13, 13]           2,320\n",
      "        LayerNorm-26           [-1, 16, 13, 13]               0\n",
      "             ReLU-27           [-1, 16, 13, 13]               0\n",
      "         Upsample-28           [-1, 16, 13, 13]               0\n",
      "  ConvTranspose2d-29           [-1, 16, 28, 28]           4,112\n",
      "        LayerNorm-30           [-1, 16, 28, 28]               0\n",
      "  ConvTranspose2d-31            [-1, 1, 28, 28]              17\n",
      "          Sigmoid-32            [-1, 1, 28, 28]               0\n",
      "      ConvDecoder-33            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 38,329\n",
      "Trainable params: 38,329\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.60\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(AE, data.shape[1::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b78ba6d8-1b4c-428f-9de3-f5233606c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 3.5310e-02, validation loss: 2.1251e-02, learning rate: 9.5000e-03\n",
      "epoch: 1, training loss: 2.0608e-02, validation loss: 1.8164e-02, learning rate: 9.0250e-03\n",
      "epoch: 2, training loss: 1.9038e-02, validation loss: 1.8383e-02, learning rate: 8.5737e-03\n",
      "epoch: 3, training loss: 1.8168e-02, validation loss: 1.8652e-02, learning rate: 8.1451e-03\n",
      "epoch: 4, training loss: 1.7609e-02, validation loss: 1.6233e-02, learning rate: 7.7378e-03\n",
      "epoch: 5, training loss: 1.7245e-02, validation loss: 1.6457e-02, learning rate: 7.3509e-03\n",
      "epoch: 6, training loss: 1.6949e-02, validation loss: 1.6566e-02, learning rate: 6.9834e-03\n",
      "epoch: 7, training loss: 1.6687e-02, validation loss: 1.6857e-02, learning rate: 6.6342e-03\n",
      "epoch: 8, training loss: 1.6508e-02, validation loss: 1.5067e-02, learning rate: 6.3025e-03\n",
      "epoch: 9, training loss: 1.6331e-02, validation loss: 1.5813e-02, learning rate: 5.9874e-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.03530957187419576,\n",
       "  0.020608120155595284,\n",
       "  0.019037541971127468,\n",
       "  0.018167791506444503,\n",
       "  0.017609122520354414,\n",
       "  0.017244840594428008,\n",
       "  0.016949333442353196,\n",
       "  0.016687417522136403,\n",
       "  0.016508295087455875,\n",
       "  0.01633064612237341],\n",
       " [tensor(0.0213, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0182, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0184, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0187, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0162, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0165, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0169, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0151, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0158, device='cuda:0', grad_fn=<MseLossBackward>)])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53550d29-8ed1-4b73-9ae3-deea2f736500",
   "metadata": {},
   "outputs": [],
   "source": [
    "### todos: check if things are working\n",
    "### add other datasets\n",
    "### convert into package\n",
    "### make optuna example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "835149dd-30bb-4173-9df0-c28c83290fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_data(data_params['dataset'],data_params['loc'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b5656c25-0e73-4956-a325-fdeedd5f851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _  = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fdcdcbc2-37b4-4c2f-b07b-00028b5eddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = AE.forward(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "169f6682-47c7-4aef-801d-66a3b89aacb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aabcfea7310>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(recon.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f9d3e494-8db5-4a08-86b1-ac66b0fb164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aabcfd09950>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(data.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b6761-0fec-4c39-a0b4-e34224084308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
