{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f65200-2db4-4ddb-9069-06f59a58b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45160da5-5cea-475f-8aeb-15f2f93352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85aa940-18bb-456c-addc-ddca10c54be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e213fc09-1720-4947-873b-ec15597ee0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ace569e-a952-48dd-b633-699fbb3c27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computes output shape for both convolutions and pooling layers\n",
    "def output_shape(in_dim,stride,padding,kernel,dilation=1):\n",
    "    out_dim = np.floor((in_dim + 2*padding - dilation*(kernel-1)-1)/stride+1).astype(int)\n",
    "    return out_dim\n",
    "\n",
    "def output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation=1):\n",
    "    out_dim = (in_dim-1)*stride-2*padding+dilation*(kernel-1)+output_padding+1\n",
    "    return out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa54417-1d5d-4ce7-9081-6359c4c8c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape(np.arange(5),stride=1,padding=0,kernel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c5ce73-6c2c-413c-a517-261aea0091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dilation(out_dim,in_dim,stride,padding,kernel,output_padding):\n",
    "    dilation = np.floor((out_dim-(in_dim-1)*stride+2*padding-output_padding-1)/(kernel-1))\n",
    "    new_dim  = output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation)\n",
    "    if new_dim == out_dim:\n",
    "        pass\n",
    "    else:\n",
    "        output_padding = (out_dim-new_dim)\n",
    "    return dilation, output_padding\n",
    "\n",
    "def get_output_padding(in_dim,out_dim, stride,padding,kernel,dilation=1):\n",
    "    return out_dim-(in_dim-1)*stride+2*padding-dilation*(kernel-1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21fa279-be16-453d-b901-f76ed2774cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce6365-5d5b-4c65-bd7b-d3762517590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['dim'] == '1D':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "099a6bc8-dc0e-4e46-957c-39e20f75fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.AdaptiveMaxPool1d#nn.MaxPool1d\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.AdaptiveMaxPool2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        current_channels   = params['input_c']\n",
    "        current_dim        = params['input_dim']\n",
    "        self.out_dims      = []\n",
    "        \n",
    "        for ii in range(nparams['n_layers']):\n",
    "            \n",
    "            conv = self.conv(current_channels, nparams['out_channels'][ii], nparams['kernel_sizes'][ii], nparams['strides'][ii], nparams['paddings'][ii])\n",
    "            self.out_dims.append(current_dim)\n",
    "            self.model.append(conv)\n",
    "            \n",
    "            current_channels =  nparams['out_channels'][ii]\n",
    "            current_dim      =  output_shape(current_dim, nparams['strides'][ii], nparams['paddings'][ii],nparams['kernel_sizes'][ii])\n",
    "            \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            pool = self.pool([current_dim//nparams['scale_facs'][ii]]*self.N)\n",
    "            self.model.append(pool)\n",
    "            \n",
    "            current_dim = current_dim//nparams['scale_facs'][ii]\n",
    "\n",
    "        self.final_dim = current_dim\n",
    "        self.final_c   = current_channels\n",
    "        \n",
    "        self.model.append(nn.Flatten())\n",
    "        current_shape = current_channels*current_dim**self.N\n",
    "        self.model.append(nn.Linear(current_shape,params['latent_dim']))\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8773f4e-9331-4073-ba73-beb8a765c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, params, nparams, out_dims, final_dim, final_c):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        if params['dim'] == '1D':\n",
    "            self.conv = nn.ConvTranspose1d\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.conv = nn.ConvTranspose2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "        \n",
    "        self.pool  = nn.Upsample\n",
    "        \n",
    "        self.model = nn.ModuleList()\n",
    "        \n",
    "        final_shape = final_c*final_dim**self.N\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        self.model.append(nn.Linear(params['latent_dim'],final_shape))\n",
    "\n",
    "        if params['dim'] == '1D':\n",
    "            self.model.append(Reshape((-1, final_c,final_dim)))\n",
    "        else:\n",
    "            self.model.append(Reshape((-1, final_c,final_dim,final_dim)))\n",
    "                              \n",
    "        current_dim      = final_dim\n",
    "        current_channels = final_c\n",
    "            \n",
    "        for jj in range(1,nparams['n_layers']+1):\n",
    "            ii = nparams['n_layers'] - jj \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "                  \n",
    "            upsample    = nn.Upsample(scale_factor=nparams['scale_facs'][ii])\n",
    "            self.model.append(upsample)\n",
    "            current_dim = current_dim*nparams['scale_facs'][ii]\n",
    "                              \n",
    "            output_padding = get_output_padding(current_dim,out_dims[ii],nparams['strides'][ii],nparams['paddings'][ii],nparams['kernel_sizes'][ii],dilation=1)\n",
    "            conv           = self.conv(current_channels, nparams['out_channels'][ii], kernel_size=nparams['kernel_sizes'][ii], stride=nparams['strides'][ii], padding=nparams['paddings'][ii], output_padding=output_padding)\n",
    "            self.model.append(conv)\n",
    "            current_channels = out_channels[ii]\n",
    "            current_dim      = output_shape_transpose(current_dim, stride=nparams['strides'][ii], padding=nparams['paddings'][ii],kernel=nparams['kernel_sizes'][ii],output_padding=output_padding)\n",
    "                \n",
    "        conv = self.conv(current_channels, 1, kernel_size=1, stride=1)\n",
    "        self.model.append(conv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1326674-3b3b-4775-9a54-7de19ac99cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0bcb7620-3c76-4891-80f9-61b62d31c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, params, network_params):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        if params['encoder_type']=='conv':\n",
    "            self.encoder = ConvEncoder\n",
    "        elif params['encoder_type']=='fc':\n",
    "            self.encoder = FCEncoder\n",
    "        else:\n",
    "            raise Exception('invalid encoder type')\n",
    "            \n",
    "        if params['decoder_type']=='conv':\n",
    "            self.decoder = ConvDecoder\n",
    "        elif params['decoder_type']=='fc':\n",
    "            self.decoder = FCDecoder\n",
    "        else:\n",
    "            raise Exception('invalid decoder type')\n",
    "        \n",
    "        self.encoder=self.encoder(params, network_params)\n",
    "        self.decoder=self.decoder(params, network_params, self.encoder.out_dims, self.encoder.final_dim, self.encoder.final_c)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3cb2599a-f4ab-4cfd-b54e-6f51cdadfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers     = 3\n",
    "out_channels = [2,4,8]\n",
    "kernel_sizes = [2,4,2]\n",
    "### stride,padding,kernel; [1,0,1] is identity\n",
    "#pooling_layers = [[1,0,1], [1,0,2]]\n",
    "scale_facs   = [2,2,1] \n",
    "paddings     = [0,0,0]\n",
    "strides      = [2,4,2]\n",
    "dim          = '1D'\n",
    "activations  = ['ReLU', 'ReLU','ReLU']\n",
    "latent_dim   = 4\n",
    "input_c      = 1 \n",
    "input_dim    = 1000\n",
    "encoder_type = 'conv'\n",
    "decoder_type = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "45c28651-e623-4d21-9038-12ece17fbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params      = {'input_c': input_c, 'input_dim': input_dim, 'latent_dim': latent_dim, 'encoder_type': encoder_type, 'decoder_type': decoder_type, 'dim': dim}\n",
    "\n",
    "conv_network_params = {'n_layers': n_layers, 'out_channels': out_channels, 'kernel_sizes': kernel_sizes, 'scale_facs': scale_facs, 'paddings': paddings,'strides': strides, 'activations': activations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1a95980-c3a4-4de6-901f-ce91aae482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = Autoencoder(general_params, conv_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d7ec4ed-be9b-43c6-b1d4-e483e534190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): ConvEncoder(\n",
       "    (model): ModuleList(\n",
       "      (0): Conv1d(1, 2, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU()\n",
       "      (2): AdaptiveMaxPool1d(output_size=[250])\n",
       "      (3): Conv1d(2, 4, kernel_size=(4,), stride=(4,))\n",
       "      (4): ReLU()\n",
       "      (5): AdaptiveMaxPool1d(output_size=[31])\n",
       "      (6): Conv1d(4, 8, kernel_size=(2,), stride=(2,))\n",
       "      (7): ReLU()\n",
       "      (8): AdaptiveMaxPool1d(output_size=[15])\n",
       "      (9): Flatten(start_dim=1, end_dim=-1)\n",
       "      (10): Linear(in_features=120, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvDecoder(\n",
       "    (model): ModuleList(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=4, out_features=120, bias=True)\n",
       "      (2): Reshape()\n",
       "      (3): ReLU()\n",
       "      (4): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (5): ConvTranspose1d(8, 8, kernel_size=(2,), stride=(2,), output_padding=(1,))\n",
       "      (6): ReLU()\n",
       "      (7): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (8): ConvTranspose1d(8, 4, kernel_size=(4,), stride=(4,), output_padding=(2,))\n",
       "      (9): ReLU()\n",
       "      (10): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (11): ConvTranspose1d(4, 2, kernel_size=(2,), stride=(2,))\n",
       "      (12): ConvTranspose1d(2, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5298e3f4-45de-49d6-b096-2c701b05a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  torch.randn(1,1,1000).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2d75e495-7dfb-49e8-95be-0c13fd32051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res = AE.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57cef670-353f-4641-9698-5b3a307bc092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ba6d8-1b4c-428f-9de3-f5233606c5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
