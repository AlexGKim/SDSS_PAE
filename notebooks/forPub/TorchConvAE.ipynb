{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f65200-2db4-4ddb-9069-06f59a58b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45160da5-5cea-475f-8aeb-15f2f93352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85aa940-18bb-456c-addc-ddca10c54be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e213fc09-1720-4947-873b-ec15597ee0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ace569e-a952-48dd-b633-699fbb3c27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computes output shape for both convolutions and pooling layers\n",
    "def output_shape(in_dim,stride,padding,kernel,dilation=1):\n",
    "    out_dim = np.floor((in_dim + 2*padding - dilation*(kernel-1)-1)/stride+1).astype(int)\n",
    "    return out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6a80ae-509a-4a06-a319-7984372387c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, out_channels, kernel_sizes, pooling_layers, paddings, strides, dim, activations, input_c, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        if dim == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.MaxPool1d\n",
    "        elif dim == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.MaxPool2d\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "        current_channels = input_c\n",
    "        current_dim      = input_dim\n",
    "        for ii in range(n_layers):\n",
    "            conv = self.conv(current_channels, out_channels[ii], kernel_sizes[ii], strides[ii], paddings[ii])\n",
    "            self.model.append(conv)\n",
    "            current_channels =  out_channels[ii]\n",
    "            current_dim      =  output_shape(current_dim, strides[ii], paddings[ii],kernel_sizes[ii])\n",
    "            \n",
    "            if activations[ii]!='None':\n",
    "                gate = getattr(nn, activations[ii])()\n",
    "                self.model.append(gate)\n",
    "                \n",
    "            if pooling_layers[ii][0]:\n",
    "                # kernel_size, stride, padding\n",
    "                pool = self.pool(pooling_layers[ii][1], pooling_layers[ii][2], pooling_layers[ii][3])\n",
    "                self.model.append(pool)\n",
    "                current_dim = output_shape(current_dim, pooling_layers[ii][2],pooling_layers[ii][3],pooling_layers[ii][1])\n",
    "                \n",
    "        self.model.append(nn.Flatten())\n",
    "        if dim == '1D':\n",
    "            current_shape = current_channels*current_dim\n",
    "        else:\n",
    "            current_shape = current_channels*current_dim**2  \n",
    "        self.model.append(nn.Linear(current_shape,latent_dim))\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a6bc8-dc0e-4e46-957c-39e20f75fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, out_channels, kernel_sizes, pooling_layers, paddings, strides, dim, activations, input_c, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        if dim == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.MaxPool1d\n",
    "        elif dim == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.MaxPool2d\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "        self.dims        = []\n",
    "        self.channels    = []\n",
    "        \n",
    "        current_channels = input_c\n",
    "        current_dim      = input_dim\n",
    "        for ii in range(n_layers):\n",
    "            conv = self.conv(current_channels, out_channels[ii], kernel_sizes[ii], strides[ii], paddings[ii])\n",
    "            self.model.append(conv)\n",
    "            current_channels =  out_channels[ii]\n",
    "            current_dim      =  output_shape(current_dim, strides[ii], paddings[ii],kernel_sizes[ii])\n",
    "            \n",
    "            if activations[ii]!='None':\n",
    "                gate = getattr(nn, activations[ii])()\n",
    "                self.model.append(gate)\n",
    "                \n",
    "            if pooling_layers[ii][0]:\n",
    "                # kernel_size, stride, padding\n",
    "                pool = self.pool(pooling_layers[ii][1], pooling_layers[ii][2], pooling_layers[ii][3])\n",
    "                self.model.append(pool)\n",
    "                current_dim = output_shape(current_dim, pooling_layers[ii][2],pooling_layers[ii][3],pooling_layers[ii][1])\n",
    "                \n",
    "        self.model.append(nn.Flatten())\n",
    "        if dim == '1D':\n",
    "            current_shape = current_channels*current_dim\n",
    "        else:\n",
    "            current_shape = current_channels*current_dim**2  \n",
    "        self.model.append(nn.Linear(current_shape,latent_dim))\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8773f4e-9331-4073-ba73-beb8a765c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, out_channels, kernel_sizes, pooling_layers, paddings, strides, dim, activations, input_c, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        if dim == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.MaxPool1d\n",
    "        elif dim == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.MaxPool2d\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "        current_channels = input_c\n",
    "        current_dim      = latent_dim\n",
    "        \n",
    "        self.model.append(nn.Linear(latent_dim))\n",
    "        \n",
    "        for ii in range(n_layers):\n",
    "            conv = self.conv(current_channels, out_channels[ii], kernel_sizes[ii], strides[ii], paddings[ii])\n",
    "            self.model.append(conv)\n",
    "            current_channels = out_channels[ii]\n",
    "            current_dim      = output_shape(current_dim, strides[ii], paddings[ii],kernel_sizes[ii])\n",
    "            \n",
    "            if activations[ii]!='None':\n",
    "                gate = getattr(nn, activations[ii])()\n",
    "                self.model.append(gate)\n",
    "                \n",
    "            if pooling_layers[ii][0]:\n",
    "                # kernel_size, stride, padding\n",
    "                pool = self.pool(pooling_layers[ii][1], pooling_layers[ii][2], pooling_layers[ii][3])\n",
    "                self.model.append(pool)\n",
    "                current_dim = output_shape(current_dim, pooling_layers[ii][2],pooling_layers[ii][3],pooling_layers[ii][1])\n",
    "                \n",
    "        self.model.append(nn.Flatten())\n",
    "        if dim == '1D':\n",
    "            current_shape = current_channels*current_dim\n",
    "        else:\n",
    "            current_shape = current_channels*current_dim**2  \n",
    "        \n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb7620-3c76-4891-80f9-61b62d31c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self, n_layers, in_channels, out_channels, kernel_sizes, pooling_layers, paddings, strides):\n",
    "#         super(Autoencoder, self).__init__()\n",
    "#         self.encoder = Encoder(n_layers, in_channels, out_channels, kernel_sizes, pooling_layers, paddings, strides)\n",
    "#         self.decoder = Decoder(n_layers, in_channels, out_channels, kernel_sizes, pooling_layers, paddings, strides)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb2599a-f4ab-4cfd-b54e-6f51cdadfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers     = 2\n",
    "out_channels = [2,4]\n",
    "kernel_sizes = [2,4]\n",
    "pooling_layers = [[False, None, None, None], [True, 2, 1, 0]]\n",
    "paddings     = [0,0]\n",
    "strides      = [2,4]\n",
    "dim          = '1D'\n",
    "activations  = ['ReLU', 'ReLU']\n",
    "latent_dim   = 4\n",
    "input_c      = 1 \n",
    "input_dim    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7ec4ed-be9b-43c6-b1d4-e483e534190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = Encoder(n_layers, out_channels, kernel_sizes, pooling_layers, paddings, strides, dim, activations, input_c, input_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5298e3f4-45de-49d6-b096-2c701b05a439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x2aaba7bba6d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d75e495-7dfb-49e8-95be-0c13fd32051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  torch.randn(1,1,100).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0848c8c-9ca2-4798-9acb-67cceb388032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0868,  0.3251, -0.1661, -0.2457]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "AE.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d45106-71ea-4fc7-97db-2dc7bea6102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1                [-1, 2, 50]               6\n",
      "              ReLU-2                [-1, 2, 50]               0\n",
      "            Conv1d-3                [-1, 4, 12]              36\n",
      "              ReLU-4                [-1, 4, 12]               0\n",
      "         MaxPool1d-5                [-1, 4, 11]               0\n",
      "           Flatten-6                   [-1, 44]               0\n",
      "            Linear-7                    [-1, 4]             180\n",
      "================================================================\n",
      "Total params: 222\n",
      "Trainable params: 222\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(AE, data.shape[1::])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f1db60-a301-4a14-94f0-fd0c919387fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Encoder' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cc8f39167094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 948\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Encoder' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "AE.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dbf0d-5e3d-405f-94ef-b67a40abb743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
