{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f65200-2db4-4ddb-9069-06f59a58b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1a59e5-b64c-4e4c-a96b-8ff0e6d7d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5d9ec3-a68a-40fa-94c2-01e1e527c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45160da5-5cea-475f-8aeb-15f2f93352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85aa940-18bb-456c-addc-ddca10c54be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e213fc09-1720-4947-873b-ec15597ee0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ace569e-a952-48dd-b633-699fbb3c27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computes output shape for both convolutions and pooling layers\n",
    "def output_shape(in_dim,stride,padding,kernel,dilation=1):\n",
    "    out_dim = np.floor((in_dim + 2*padding - dilation*(kernel-1)-1)/stride+1).astype(int)\n",
    "    return out_dim\n",
    "\n",
    "def output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation=1):\n",
    "    out_dim = (in_dim-1)*stride-2*padding+dilation*(kernel-1)+output_padding+1\n",
    "    return out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa54417-1d5d-4ce7-9081-6359c4c8c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape(np.arange(5),stride=1,padding=0,kernel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c5ce73-6c2c-413c-a517-261aea0091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dilation(out_dim,in_dim,stride,padding,kernel,output_padding):\n",
    "    dilation = np.floor((out_dim-(in_dim-1)*stride+2*padding-output_padding-1)/(kernel-1))\n",
    "    new_dim  = output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation)\n",
    "    if new_dim == out_dim:\n",
    "        pass\n",
    "    else:\n",
    "        output_padding = (out_dim-new_dim)\n",
    "    return dilation, output_padding\n",
    "\n",
    "def get_output_padding(in_dim,out_dim, stride,padding,kernel,dilation=1):\n",
    "    return out_dim-(in_dim-1)*stride+2*padding-dilation*(kernel-1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d21fa279-be16-453d-b901-f76ed2774cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a878e81-e0a4-454a-80d5-0e80c0220cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCEncoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(FCEncoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        \n",
    "        current_dim = params['input_dim']**self.N*params['input_c']\n",
    "        \n",
    "        for ii in range(nparams['n_layers']):\n",
    "            \n",
    "            lin = nn.Linear(current_dim, nparams['out_sizes'][ii])\n",
    "            self.model.append(spec_norm(lin))\n",
    "            \n",
    "            current_dim      =  nparams['out_sizes'][ii]\n",
    "            \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm(current_dim,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)\n",
    "            \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            dropout = nn.Dropout(nparams['dropout_rate'][ii])\n",
    "            self.model.append(dropout)\n",
    "        \n",
    "        lin = nn.Linear(current_dim,params['latent_dim'])\n",
    "        self.model.append(spec_norm(lin))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099a6bc8-dc0e-4e46-957c-39e20f75fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.AdaptiveMaxPool1d#nn.MaxPool1d\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.AdaptiveMaxPool2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        current_channels   = params['input_c']\n",
    "        current_dim        = params['input_dim']\n",
    "        self.out_dims      = []\n",
    "        \n",
    "        for ii in range(nparams['n_layers']):\n",
    "            \n",
    "            conv = self.conv(current_channels, nparams['out_channels'][ii], nparams['kernel_sizes'][ii], nparams['strides'][ii], nparams['paddings'][ii])\n",
    "            self.out_dims.append(current_dim)\n",
    "            self.model.append(spec_norm(conv))\n",
    "            \n",
    "            current_channels =  nparams['out_channels'][ii]\n",
    "            current_dim      =  output_shape(current_dim, nparams['strides'][ii], nparams['paddings'][ii],nparams['kernel_sizes'][ii])\n",
    "            \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm([current_channels]+[current_dim]*self.N,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)\n",
    "                \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            pool = self.pool([current_dim//nparams['scale_facs'][ii]]*self.N)\n",
    "            self.model.append(pool)\n",
    "            \n",
    "            current_dim = current_dim//nparams['scale_facs'][ii]\n",
    "\n",
    "        self.final_dim = current_dim\n",
    "        self.final_c   = current_channels\n",
    "        \n",
    "        self.model.append(nn.Flatten())\n",
    "        current_shape = current_channels*current_dim**self.N\n",
    "        linear        = nn.Linear(current_shape,params['latent_dim'])\n",
    "        self.model.append(spec_norm(linear))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "\n",
    "            x = l(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8773f4e-9331-4073-ba73-beb8a765c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        if params['dim'] == '1D':\n",
    "            self.conv = nn.ConvTranspose1d\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.conv = nn.ConvTranspose2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "        \n",
    "        self.pool   = nn.Upsample\n",
    "        \n",
    "        self.model  = nn.ModuleList()\n",
    "        \n",
    "        final_shape = nparams['final_c']*nparams['final_dim']**self.N\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        lin         = nn.Linear(params['latent_dim'],final_shape)\n",
    "        self.model.append(spec_norm(lin))\n",
    "\n",
    "        if params['dim'] == '1D':\n",
    "            self.model.append(Reshape((-1, nparams['final_c'],nparams['final_dim'])))\n",
    "        else:\n",
    "            self.model.append(Reshape((-1, nparams['final_c'],nparams['final_dim'],nparams['final_dim'])))\n",
    "                              \n",
    "        current_dim      = nparams['final_dim']\n",
    "        current_channels = nparams['final_c']\n",
    "            \n",
    "        for jj in range(1,nparams['n_layers']+1):\n",
    "            ii = nparams['n_layers'] - jj \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "                  \n",
    "            upsample    = nn.Upsample(scale_factor=nparams['scale_facs'][ii])\n",
    "            self.model.append(upsample)\n",
    "            current_dim = current_dim*nparams['scale_facs'][ii]\n",
    "                              \n",
    "            output_padding = get_output_padding(current_dim,nparams['out_dims'][ii],nparams['strides'][ii],nparams['paddings'][ii],nparams['kernel_sizes'][ii],dilation=1)\n",
    "            conv           = self.conv(current_channels, nparams['out_channels'][ii], kernel_size=nparams['kernel_sizes'][ii], stride=nparams['strides'][ii], padding=nparams['paddings'][ii], output_padding=output_padding)\n",
    "            self.model.append(spec_norm(conv))\n",
    "            \n",
    "            current_channels = nparams['out_channels'][ii]\n",
    "            current_dim      = output_shape_transpose(current_dim, stride=nparams['strides'][ii], padding=nparams['paddings'][ii],kernel=nparams['kernel_sizes'][ii],output_padding=output_padding)\n",
    "                \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm([current_channels]+[current_dim]*self.N,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)    \n",
    "                \n",
    "        \n",
    "        conv = self.conv(current_channels, 1, kernel_size=1, stride=1)\n",
    "        self.model.append(spec_norm(conv))\n",
    "        \n",
    "        if nparams['image_data']: \n",
    "            self.model.append(getattr(nn, 'Sigmoid')())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1326674-3b3b-4775-9a54-7de19ac99cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDecoder(nn.Module):\n",
    "    def __init__(self, params, nparams):\n",
    "        super(FCDecoder, self).__init__()\n",
    "        if params['dim'] == '1D':\n",
    "            self.N    = 1\n",
    "        elif params['dim'] == '2D':\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "            \n",
    "        if nparams['spec_norm']:\n",
    "            spec_norm = nn.utils.spectral_norm\n",
    "        else:\n",
    "            spec_norm = nn.Identity()\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        \n",
    "        current_dim = params['latent_dim']\n",
    "        \n",
    "        for jj in range(1,nparams['n_layers']+1):\n",
    "            ii = nparams['n_layers'] - jj \n",
    "            \n",
    "            lin = nn.Linear(current_dim, nparams['out_sizes'][ii])\n",
    "            self.model.append(spec_norm(lin))\n",
    "            \n",
    "            current_dim      =  nparams['out_sizes'][ii]\n",
    "            \n",
    "            if nparams['layer_norm'][ii]:\n",
    "                norm = nn.LayerNorm(current_dim,elementwise_affine=nparams['affine'])\n",
    "                self.model.append(norm)\n",
    "                \n",
    "            gate = getattr(nn, nparams['activations'][ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            dropout = nn.Dropout(nparams['dropout_rate'][ii])\n",
    "            self.model.append(dropout)\n",
    "        \n",
    "        lin = nn.Linear(current_dim,params['input_dim']**self.N*params['input_c'])\n",
    "        self.model.append(spec_norm(lin))\n",
    "        \n",
    "        if nparams['image_data']: \n",
    "            self.model.append(getattr(nn, 'Sigmoid')())\n",
    "        \n",
    "        self.model.append(Reshape([-1]+[params['input_c']]+[params['input_dim']]*self.N))\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b7aae5-9771-4a32-99f7-823bc046a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, loc, batchsize):\n",
    "    \n",
    "    if data in dir(datasets):\n",
    "        dataset = getattr(datasets,data)\n",
    "    \n",
    "        training_data = dataset(root=loc,train=True,download=True,transform=ToTensor())\n",
    "\n",
    "        valid_data    = dataset(root=loc,train=False,download=True,transform=ToTensor())\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    train_dataloader = DataLoader(training_data, batch_size=batchsize, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size=256, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bcb7620-3c76-4891-80f9-61b62d31c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, params, dparams, nparams_enc, nparams_dec, tparams):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        if params['encoder_type'] == 'conv':\n",
    "            self.encoder = ConvEncoder(params, nparams_enc)\n",
    "            nparams_enc['out_dims']  = self.encoder.out_dims\n",
    "            nparams_enc['final_dim'] = self.encoder.final_dim\n",
    "            nparams_enc['final_c']   = self.encoder.final_c\n",
    "        elif params['encoder_type'] == 'fc':\n",
    "            self.encoder = FCEncoder(params, nparams_enc)\n",
    "        else:\n",
    "            raise Exception('invalid encoder type')\n",
    "            \n",
    "        if params['decoder_type'] == 'conv':\n",
    "            self.decoder = ConvDecoder(params, nparams_dec)\n",
    "        elif params['decoder_type'] == 'fc':\n",
    "            self.decoder = FCDecoder(params, nparams_dec)\n",
    "        else:\n",
    "            raise Exception('invalid decoder type')\n",
    "        \n",
    "        self.optimizer = getattr(optim, tparams['optimizer'])\n",
    "        self.optimizer = self.optimizer(self.parameters(),tparams['initial_lr'])\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, tparams['scheduler']),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**tparams['scheduler_params'])\n",
    "        \n",
    "        self.criterion = getattr(nn, tparams['criterion'])()\n",
    "        \n",
    "        self.train_loader, valid_loader = get_data(dparams['dataset'],dparams['loc'],tparams['batchsize'])\n",
    "        \n",
    "        self.valid_loader = iter(valid_loader)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def update_lr(self,lr):\n",
    "    \n",
    "        self.optimizer = getattr(optim, tparams['optimizer'])\n",
    "        self.optimizer  = self.optimizer(self.parameters(),lr)\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, tparams['scheduler']),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**tparams['scheduler_params'])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def update_scheduler(self,scheduler, scheduler_params):\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, scheduler),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**scheduler_params)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def update_optimizer(self,optimizer):\n",
    "        \n",
    "        self.optimizer = getattr(optim, optimizer)\n",
    "        self.optimizer  = self.optimizer(self.parameters(),tparams['initial_lr'])\n",
    "        \n",
    "        self.scheduler = partial(getattr(torch.optim.lr_scheduler, tparams['scheduler']),self.optimizer)\n",
    "        self.scheduler = self.scheduler(**tparams['scheduler_params'])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def train(self, nepochs):\n",
    "        running_loss    = []\n",
    "        validation_loss = []\n",
    "        for epoch in range(nepochs):\n",
    "            r_loss = 0\n",
    "            for ii, (data, _) in enumerate(self.train_loader,0):\n",
    "                data = data.to(device)\n",
    "                recon = self.forward(data)\n",
    "                loss  = self.criterion(recon, data)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                r_loss+=loss.item()\n",
    "            self.scheduler.step()\n",
    "            running_loss.append(r_loss/ii)\n",
    "            valid_data, _ = next(self.valid_loader)\n",
    "            valid_data = valid_data.to(device)\n",
    "            recon      = self.forward(valid_data)\n",
    "            loss       = self.criterion(recon, valid_data)\n",
    "            validation_loss.append(loss)\n",
    "            print(f'epoch: {epoch:d}, training loss: {running_loss[-1]:.4e}, validation loss: {loss:.4e}, learning rate: {self.scheduler.get_last_lr()[0]:.4e}')\n",
    "        return running_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb2599a-f4ab-4cfd-b54e-6f51cdadfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers     = 3\n",
    "out_channels = [16,16,16]\n",
    "kernel_sizes = [4,3,2]\n",
    "### stride,padding,kernel; [1,0,1] is identity\n",
    "#pooling_layers = [[1,0,1], [1,0,2]]\n",
    "scale_facs   = [1,1,1] \n",
    "paddings     = [0,0,0]\n",
    "strides      = [2,1,1]\n",
    "layer_norm   = [True,True,True]\n",
    "dropout_rate = [0.,0.,0.]\n",
    "spec_norm    = True\n",
    "dim          = '2D'\n",
    "activations  = ['ReLU', 'ReLU','ReLU']\n",
    "latent_dim   = 8\n",
    "input_c      = 1 \n",
    "input_dim    = 28\n",
    "encoder_type = 'conv'\n",
    "decoder_type = 'conv'\n",
    "affine       = False\n",
    "out_sizes    = [256,128,64]\n",
    "image_data   = True\n",
    "\n",
    "nepochs       = 20\n",
    "batchsize     = 64\n",
    "initial_lr    = 1e-2\n",
    "\n",
    "optimizer     = 'Adam'\n",
    "criterion     = 'MSELoss'\n",
    "\n",
    "scheduler     = 'ExponentialLR'\n",
    "scheduler_params = {'gamma':0.95}\n",
    "\n",
    "dataset       = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "45c28651-e623-4d21-9038-12ece17fbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params      = {'input_c': input_c, 'input_dim': input_dim, 'latent_dim': latent_dim, 'encoder_type': encoder_type, 'decoder_type': decoder_type, 'dim': dim}\n",
    "conv_network_params = {'n_layers': n_layers, 'out_channels': out_channels, 'kernel_sizes': kernel_sizes, 'scale_facs': scale_facs, 'paddings': paddings,\\\n",
    "                       'strides': strides,'activations': activations, 'spec_norm': spec_norm, 'dropout_rate':dropout_rate, 'layer_norm': layer_norm,\\\n",
    "                       'affine': affine,'image_data': image_data}\n",
    "fc_network_params   = {'n_layers': n_layers, 'out_sizes': out_sizes,'activations': activations, 'spec_norm': spec_norm, 'dropout_rate':dropout_rate, \\\n",
    "                       'layer_norm': layer_norm, 'affine': affine, 'image_data': image_data}\n",
    "\n",
    "training_params     = {'batchsize': batchsize, 'initial_lr': initial_lr, 'optimizer': optimizer, 'criterion': criterion, \\\n",
    "                       'scheduler': scheduler, 'scheduler_params':scheduler_params}\n",
    "data_params         = {'dataset':dataset, 'loc': '/global/cscratch1/sd/vboehm/Datasets'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b1a95980-c3a4-4de6-901f-ce91aae482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = Autoencoder(general_params,data_params,conv_network_params, conv_network_params, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1af0f28d-bc4f-44f3-999d-97a04331095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(AE.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9604fb11-3562-4b6f-9bfd-c14bf8ef035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8d7ec4ed-be9b-43c6-b1d4-e483e534190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): ConvEncoder(\n",
       "    (model): ModuleList(\n",
       "      (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): LayerNorm((16, 13, 13), eps=1e-05, elementwise_affine=False)\n",
       "      (2): ReLU()\n",
       "      (3): AdaptiveMaxPool2d(output_size=[13, 13])\n",
       "      (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): LayerNorm((16, 11, 11), eps=1e-05, elementwise_affine=False)\n",
       "      (6): ReLU()\n",
       "      (7): AdaptiveMaxPool2d(output_size=[11, 11])\n",
       "      (8): Conv2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (9): LayerNorm((16, 10, 10), eps=1e-05, elementwise_affine=False)\n",
       "      (10): ReLU()\n",
       "      (11): AdaptiveMaxPool2d(output_size=[10, 10])\n",
       "      (12): Flatten(start_dim=1, end_dim=-1)\n",
       "      (13): Linear(in_features=1600, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvDecoder(\n",
       "    (model): ModuleList(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=8, out_features=1600, bias=True)\n",
       "      (2): Reshape()\n",
       "      (3): ReLU()\n",
       "      (4): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (5): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (6): LayerNorm((16, 11, 11), eps=1e-05, elementwise_affine=False)\n",
       "      (7): ReLU()\n",
       "      (8): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (9): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (10): LayerNorm((16, 13, 13), eps=1e-05, elementwise_affine=False)\n",
       "      (11): ReLU()\n",
       "      (12): Upsample(scale_factor=1.0, mode=nearest)\n",
       "      (13): ConvTranspose2d(16, 16, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (14): LayerNorm((16, 28, 28), eps=1e-05, elementwise_affine=False)\n",
       "      (15): ConvTranspose2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (16): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5298e3f4-45de-49d6-b096-2c701b05a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  torch.randn(1,1,28,28).to('cuda')\n",
    "\n",
    "\n",
    "res = AE.forward(train_features.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9175fd50-35ec-416a-8f8e-21ff06de672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 13, 13]             272\n",
      "         LayerNorm-2           [-1, 16, 13, 13]               0\n",
      "              ReLU-3           [-1, 16, 13, 13]               0\n",
      " AdaptiveMaxPool2d-4           [-1, 16, 13, 13]               0\n",
      "            Conv2d-5           [-1, 16, 11, 11]           2,320\n",
      "         LayerNorm-6           [-1, 16, 11, 11]               0\n",
      "              ReLU-7           [-1, 16, 11, 11]               0\n",
      " AdaptiveMaxPool2d-8           [-1, 16, 11, 11]               0\n",
      "            Conv2d-9           [-1, 16, 10, 10]           1,040\n",
      "        LayerNorm-10           [-1, 16, 10, 10]               0\n",
      "             ReLU-11           [-1, 16, 10, 10]               0\n",
      "AdaptiveMaxPool2d-12           [-1, 16, 10, 10]               0\n",
      "          Flatten-13                 [-1, 1600]               0\n",
      "           Linear-14                    [-1, 8]          12,808\n",
      "      ConvEncoder-15                    [-1, 8]               0\n",
      "          Flatten-16                    [-1, 8]               0\n",
      "           Linear-17                 [-1, 1600]          14,400\n",
      "          Reshape-18           [-1, 16, 10, 10]               0\n",
      "             ReLU-19           [-1, 16, 10, 10]               0\n",
      "         Upsample-20           [-1, 16, 10, 10]               0\n",
      "  ConvTranspose2d-21           [-1, 16, 11, 11]           1,040\n",
      "        LayerNorm-22           [-1, 16, 11, 11]               0\n",
      "             ReLU-23           [-1, 16, 11, 11]               0\n",
      "         Upsample-24           [-1, 16, 11, 11]               0\n",
      "  ConvTranspose2d-25           [-1, 16, 13, 13]           2,320\n",
      "        LayerNorm-26           [-1, 16, 13, 13]               0\n",
      "             ReLU-27           [-1, 16, 13, 13]               0\n",
      "         Upsample-28           [-1, 16, 13, 13]               0\n",
      "  ConvTranspose2d-29           [-1, 16, 28, 28]           4,112\n",
      "        LayerNorm-30           [-1, 16, 28, 28]               0\n",
      "  ConvTranspose2d-31            [-1, 1, 28, 28]              17\n",
      "          Sigmoid-32            [-1, 1, 28, 28]               0\n",
      "      ConvDecoder-33            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 38,329\n",
      "Trainable params: 38,329\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.60\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(AE, data.shape[1::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b78ba6d8-1b4c-428f-9de3-f5233606c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 3.0870e-02, validation loss: 2.1789e-02, learning rate: 9.5000e-03\n",
      "epoch: 1, training loss: 2.0042e-02, validation loss: 1.7576e-02, learning rate: 9.0250e-03\n",
      "epoch: 2, training loss: 1.8681e-02, validation loss: 1.7464e-02, learning rate: 8.5737e-03\n",
      "epoch: 3, training loss: 1.7910e-02, validation loss: 1.6586e-02, learning rate: 8.1451e-03\n",
      "epoch: 4, training loss: 1.7364e-02, validation loss: 1.7372e-02, learning rate: 7.7378e-03\n",
      "epoch: 5, training loss: 1.6969e-02, validation loss: 1.6563e-02, learning rate: 7.3509e-03\n",
      "epoch: 6, training loss: 1.6660e-02, validation loss: 1.7290e-02, learning rate: 6.9834e-03\n",
      "epoch: 7, training loss: 1.6401e-02, validation loss: 1.6382e-02, learning rate: 6.6342e-03\n",
      "epoch: 8, training loss: 1.6219e-02, validation loss: 1.7758e-02, learning rate: 6.3025e-03\n",
      "epoch: 9, training loss: 1.6041e-02, validation loss: 1.6339e-02, learning rate: 5.9874e-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0308703372837417,\n",
       "  0.02004217982427351,\n",
       "  0.018680692397916775,\n",
       "  0.01790957682017583,\n",
       "  0.017363673610773768,\n",
       "  0.016969170037700754,\n",
       "  0.016659738024817967,\n",
       "  0.016401116133905082,\n",
       "  0.016218815382526296,\n",
       "  0.016041004856334744],\n",
       " [tensor(0.0218, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0176, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0175, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0174, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0173, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0164, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       "  tensor(0.0163, device='cuda:0', grad_fn=<MseLossBackward>)])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss, valid_loss = AE.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "53550d29-8ed1-4b73-9ae3-deea2f736500",
   "metadata": {},
   "outputs": [],
   "source": [
    "### todos: check if things are working\n",
    "### add other datasets\n",
    "### convert into package\n",
    "### make optuna example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d6da664f-824c-418d-b307-ad55d2df4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_data(data_params['dataset'],data_params['loc'],1)\n",
    "\n",
    "data, _  = next(iter(train_loader))\n",
    "\n",
    "recon = AE.forward(data.to(device))\n",
    "\n",
    "plt.imshow(np.squeeze(recon.cpu().detach().numpy()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f9d3e494-8db5-4a08-86b1-ac66b0fb164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2aabafecf450>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(data.cpu().detach().numpy()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b6761-0fec-4c39-a0b4-e34224084308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
