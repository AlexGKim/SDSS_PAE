{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f65200-2db4-4ddb-9069-06f59a58b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45160da5-5cea-475f-8aeb-15f2f93352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85aa940-18bb-456c-addc-ddca10c54be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e213fc09-1720-4947-873b-ec15597ee0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ace569e-a952-48dd-b633-699fbb3c27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computes output shape for both convolutions and pooling layers\n",
    "def output_shape(in_dim,stride,padding,kernel,dilation=1):\n",
    "    out_dim = np.floor((in_dim + 2*padding - dilation*(kernel-1)-1)/stride+1).astype(int)\n",
    "    return out_dim\n",
    "\n",
    "def output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation=1):\n",
    "    out_dim = (in_dim-1)*stride-2*padding+dilation*(kernel-1)+output_padding+1\n",
    "    return out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa54417-1d5d-4ce7-9081-6359c4c8c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape(np.arange(5),stride=1,padding=0,kernel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c5ce73-6c2c-413c-a517-261aea0091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dilation(out_dim,in_dim,stride,padding,kernel,output_padding):\n",
    "    dilation = np.floor((out_dim-(in_dim-1)*stride+2*padding-output_padding-1)/(kernel-1))\n",
    "    new_dim  = output_shape_transpose(in_dim,stride,padding,kernel,output_padding, dilation)\n",
    "    if new_dim == out_dim:\n",
    "        pass\n",
    "    else:\n",
    "        output_padding = (out_dim-new_dim)\n",
    "    return dilation, output_padding\n",
    "\n",
    "def get_output_padding(in_dim,out_dim, stride,padding,kernel,dilation=1):\n",
    "    return out_dim-(in_dim-1)*stride+2*padding-dilation*(kernel-1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21fa279-be16-453d-b901-f76ed2774cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "099a6bc8-dc0e-4e46-957c-39e20f75fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, input_c, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        if dim == '1D':\n",
    "            self.conv = nn.Conv1d\n",
    "            self.pool = nn.AdaptiveMaxPool1d#nn.MaxPool1d\n",
    "            self.N    = 1\n",
    "        elif dim == '2D':\n",
    "            self.conv = nn.Conv2d\n",
    "            self.pool = nn.AdaptiveMaxPool2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "            \n",
    "        self.model = nn.ModuleList()\n",
    "    \n",
    "        current_channels   = input_c\n",
    "        current_dim        = input_dim\n",
    "        self.out_dims      = []\n",
    "        \n",
    "        for ii in range(n_layers):\n",
    "            \n",
    "            conv = self.conv(current_channels, out_channels[ii], kernel_sizes[ii], strides[ii], paddings[ii])\n",
    "            self.out_dims.append(current_dim)\n",
    "            self.model.append(conv)\n",
    "            \n",
    "            current_channels =  out_channels[ii]\n",
    "            current_dim      =  output_shape(current_dim, strides[ii], paddings[ii],kernel_sizes[ii])\n",
    "            \n",
    "            gate = getattr(nn, activations[ii])()\n",
    "            self.model.append(gate)\n",
    "            \n",
    "            pool = self.pool([current_dim//scale_facs[ii]]*self.N)\n",
    "            self.model.append(pool)\n",
    "            \n",
    "            current_dim = current_dim//scale_facs[ii]\n",
    "\n",
    "        self.final_dim = current_dim\n",
    "        self.final_c   = current_channels\n",
    "        \n",
    "        self.model.append(nn.Flatten())\n",
    "        current_shape = current_channels*current_dim**self.N\n",
    "        self.model.append(nn.Linear(current_shape,latent_dim))\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            print(l, x.shape)\n",
    "            x = l(x)\n",
    "            print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b8773f4e-9331-4073-ba73-beb8a765c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, latent_dim, final_dim, final_c, out_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        if dim == '1D':\n",
    "            self.conv = nn.ConvTranspose1d\n",
    "            self.N    = 1\n",
    "        elif dim == '2D':\n",
    "            self.conv = nn.ConvTranspose2d\n",
    "            self.N    = 2\n",
    "        else:\n",
    "            raise Exception(\"Invalid data dimensionality (must be either 1D or 2D).\")\n",
    "        \n",
    "        self.pool  = nn.Upsample\n",
    "        \n",
    "        self.model = nn.ModuleList()\n",
    "        \n",
    "        final_shape = final_c*final_dim**self.N\n",
    "    \n",
    "        self.model.append(nn.Flatten())\n",
    "        self.model.append(nn.Linear(latent_dim,final_shape))\n",
    "\n",
    "        if dim == '1D':\n",
    "            self.model.append(Reshape((-1, final_c,final_dim)))\n",
    "        else:\n",
    "            self.model.append(Reshape((-1, final_c,final_dim,final_dim)))\n",
    "                              \n",
    "        current_dim      = final_dim\n",
    "        current_channels = final_c\n",
    "            \n",
    "        for jj in range(1,n_layers+1):\n",
    "            ii = n_layers - jj \n",
    "            gate = getattr(nn, activations[ii])()\n",
    "            self.model.append(gate)\n",
    "                  \n",
    "            upsample    = nn.Upsample(scale_factor=scale_facs[ii])\n",
    "            self.model.append(upsample)\n",
    "            current_dim = current_dim*scale_facs[ii]\n",
    "                              \n",
    "            output_padding = get_output_padding(current_dim,out_dims[ii],strides[ii],paddings[ii],kernel_sizes[ii],dilation=1)\n",
    "            conv           = self.conv(current_channels, out_channels[ii], kernel_size=kernel_sizes[ii], stride=strides[ii], padding=paddings[ii], output_padding=output_padding)\n",
    "            self.model.append(conv)\n",
    "            current_channels = out_channels[ii]\n",
    "            current_dim      = output_shape_transpose(current_dim, stride=strides[ii], padding=paddings[ii],kernel=kernel_sizes[ii],output_padding=output_padding)\n",
    "                \n",
    "        conv = self.conv(current_channels, 1, kernel_size=1, stride=1)\n",
    "        self.model.append(conv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        for i, l in enumerate(self.model):\n",
    "            print(x.shape, l)\n",
    "            x = l(x)\n",
    "            print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0bcb7620-3c76-4891-80f9-61b62d31c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, input_c, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, input_c, input_dim, latent_dim)\n",
    "        self.decoder = Decoder(n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, latent_dim, self.encoder.final_dim, self.encoder.final_c, self.encoder.out_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3cb2599a-f4ab-4cfd-b54e-6f51cdadfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers     = 2\n",
    "out_channels = [2,4]\n",
    "kernel_sizes = [2,4]\n",
    "scale_facs   = [1,4]\n",
    "### stride,padding,kernel; [1,0,1] is identity\n",
    "#pooling_layers = [[1,0,1], [1,0,2]]\n",
    "scale_facs   = [2,1] \n",
    "paddings     = [0,0]\n",
    "strides      = [2,4]\n",
    "dim          = '2D'\n",
    "activations  = ['ReLU', 'ReLU']\n",
    "latent_dim   = 4\n",
    "input_c      = 1 \n",
    "input_dim    = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b1a95980-c3a4-4de6-901f-ce91aae482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = Autoencoder(n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, input_c, input_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8d7ec4ed-be9b-43c6-b1d4-e483e534190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Enc = Encoder(n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, input_c, input_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5298e3f4-45de-49d6-b096-2c701b05a439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (model): ModuleList(\n",
       "    (0): Conv2d(1, 2, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveMaxPool2d(output_size=[250, 250])\n",
       "    (3): Conv2d(2, 4, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (4): ReLU()\n",
       "    (5): AdaptiveMaxPool2d(output_size=[62, 62])\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=15376, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2d75e495-7dfb-49e8-95be-0c13fd32051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  torch.randn(1,1,1000,1000).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d0848c8c-9ca2-4798-9acb-67cceb388032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 2, kernel_size=(2, 2), stride=(2, 2)) torch.Size([1, 1, 1000, 1000])\n",
      "torch.Size([1, 2, 500, 500])\n",
      "ReLU() torch.Size([1, 2, 500, 500])\n",
      "torch.Size([1, 2, 500, 500])\n",
      "AdaptiveMaxPool2d(output_size=[250, 250]) torch.Size([1, 2, 500, 500])\n",
      "torch.Size([1, 2, 250, 250])\n",
      "Conv2d(2, 4, kernel_size=(4, 4), stride=(4, 4)) torch.Size([1, 2, 250, 250])\n",
      "torch.Size([1, 4, 62, 62])\n",
      "ReLU() torch.Size([1, 4, 62, 62])\n",
      "torch.Size([1, 4, 62, 62])\n",
      "AdaptiveMaxPool2d(output_size=[62, 62]) torch.Size([1, 4, 62, 62])\n",
      "torch.Size([1, 4, 62, 62])\n",
      "Flatten(start_dim=1, end_dim=-1) torch.Size([1, 4, 62, 62])\n",
      "torch.Size([1, 15376])\n",
      "Linear(in_features=15376, out_features=4, bias=True) torch.Size([1, 15376])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "res = Enc.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4406c929-0ab7-4e99-9f66-fc06bbfbc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dec =  Decoder(n_layers, out_channels, kernel_sizes, scale_facs, paddings, strides, dim, activations, latent_dim, Enc.final_dim, Enc.final_c, Enc.out_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3e13bcbb-8457-4cc0-8ba6-3e17697953bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (model): ModuleList(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=4, out_features=15376, bias=True)\n",
       "    (2): Reshape()\n",
       "    (3): ReLU()\n",
       "    (4): Upsample(scale_factor=1.0, mode=nearest)\n",
       "    (5): ConvTranspose2d(4, 4, kernel_size=(4, 4), stride=(4, 4), output_padding=(2, 2))\n",
       "    (6): ReLU()\n",
       "    (7): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (8): ConvTranspose2d(4, 2, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (9): ConvTranspose2d(2, 1, kernel_size=(2, 2), stride=(0, 0))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "16cec0c7-b16c-4a36-9d42-e394cebb7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4]) Flatten(start_dim=1, end_dim=-1)\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4]) Linear(in_features=4, out_features=15376, bias=True)\n",
      "torch.Size([1, 15376])\n",
      "torch.Size([1, 15376]) Reshape()\n",
      "torch.Size([1, 4, 62, 62])\n",
      "torch.Size([1, 4, 62, 62]) ReLU()\n",
      "torch.Size([1, 4, 62, 62])\n",
      "torch.Size([1, 4, 62, 62]) Upsample(scale_factor=1.0, mode=nearest)\n",
      "torch.Size([1, 4, 62, 62])\n",
      "torch.Size([1, 4, 62, 62]) ConvTranspose2d(4, 4, kernel_size=(4, 4), stride=(4, 4), output_padding=(2, 2))\n",
      "torch.Size([1, 4, 250, 250])\n",
      "torch.Size([1, 4, 250, 250]) ReLU()\n",
      "torch.Size([1, 4, 250, 250])\n",
      "torch.Size([1, 4, 250, 250]) Upsample(scale_factor=2.0, mode=nearest)\n",
      "torch.Size([1, 4, 500, 500])\n",
      "torch.Size([1, 4, 500, 500]) ConvTranspose2d(4, 2, kernel_size=(2, 2), stride=(2, 2))\n",
      "torch.Size([1, 2, 1000, 1000])\n",
      "torch.Size([1, 2, 1000, 1000]) ConvTranspose2d(2, 1, kernel_size=(2, 2), stride=(0, 0))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "non-positive stride is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-797948ebd8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-230-2add22bcfc11>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    840\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    841\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: non-positive stride is not supported"
     ]
    }
   ],
   "source": [
    "Dec.forward(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "35d45106-71ea-4fc7-97db-2dc7bea6102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1])\n",
      "torch.Size([2, 4, 1]) Flatten(start_dim=1, end_dim=-1)\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4]) Linear(in_features=4, out_features=15376, bias=True)\n",
      "torch.Size([2, 15376])\n",
      "torch.Size([2, 15376]) Reshape()\n",
      "torch.Size([2, 4, 62, 62])\n",
      "torch.Size([2, 4, 62, 62]) ReLU()\n",
      "torch.Size([2, 4, 62, 62])\n",
      "torch.Size([2, 4, 62, 62]) Upsample(scale_factor=1.0, mode=nearest)\n",
      "torch.Size([2, 4, 62, 62])\n",
      "torch.Size([2, 4, 62, 62]) ConvTranspose2d(4, 4, kernel_size=(4, 4), stride=(4, 4), output_padding=(2, 2))\n",
      "torch.Size([2, 4, 250, 250])\n",
      "torch.Size([2, 4, 250, 250]) ReLU()\n",
      "torch.Size([2, 4, 250, 250])\n",
      "torch.Size([2, 4, 250, 250]) Upsample(scale_factor=2.0, mode=nearest)\n",
      "torch.Size([2, 4, 500, 500])\n",
      "torch.Size([2, 4, 500, 500]) ConvTranspose2d(4, 2, kernel_size=(2, 2), stride=(2, 2))\n",
      "torch.Size([2, 2, 1000, 1000])\n",
      "torch.Size([2, 2, 1000, 1000]) ConvTranspose2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "torch.Size([2, 1, 1000, 1000])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                    [-1, 4]               0\n",
      "            Linear-2                [-1, 15376]          76,880\n",
      "           Reshape-3            [-1, 4, 62, 62]               0\n",
      "              ReLU-4            [-1, 4, 62, 62]               0\n",
      "          Upsample-5            [-1, 4, 62, 62]               0\n",
      "   ConvTranspose2d-6          [-1, 4, 250, 250]             260\n",
      "              ReLU-7          [-1, 4, 250, 250]               0\n",
      "          Upsample-8          [-1, 4, 500, 500]               0\n",
      "   ConvTranspose2d-9        [-1, 2, 1000, 1000]              34\n",
      "  ConvTranspose2d-10        [-1, 1, 1000, 1000]               3\n",
      "================================================================\n",
      "Total params: 77,177\n",
      "Trainable params: 77,177\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 34.80\n",
      "Params size (MB): 0.29\n",
      "Estimated Total Size (MB): 35.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(Dec, (4,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1db60-a301-4a14-94f0-fd0c919387fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dbf0d-5e3d-405f-94ef-b67a40abb743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
