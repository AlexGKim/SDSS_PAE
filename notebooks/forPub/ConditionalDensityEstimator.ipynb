{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b0d9a8c-6096-4ce1-8833-dfe6f906f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ea81882-3847-42de-84b3-750a1325be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e26e956f-c6a9-423c-b051-110de9197dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/global/u2/v/vboehm/codes/SIG_GIS/')\n",
    "from sig_gis import *\n",
    "from sig_gis.GIS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "624a77ee-66c4-4dbc-a899-d0147d5c061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d40683e-c543-4b19-8dc2-095c547b12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Samplers = {'SMOTE':SMOTE,'RandomOverSampler':RandomOverSampler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6227469-0c2f-4817-93fe-6e02a4423976",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN             = '1'\n",
    "EPOCHS          = 40\n",
    "\n",
    "seeds           = {'1':512, '2':879, '3':9981, '4': 20075, '5': 66, '6': 276, '7': 936664}\n",
    "\n",
    "conditional     = False\n",
    "cond_on         = 'type'\n",
    "\n",
    "root_model_data = '/global/cscratch1/sd/vboehm/Datasets/sdss/by_model/'\n",
    "root_models     = '/global/cscratch1/sd/vboehm/Models/SDSS_AE/'\n",
    "root_encoded    = '/global/cscratch1/sd/vboehm/Datasets/encoded/sdss/'\n",
    "root_decoded    = '/global/cscratch1/sd/vboehm/Datasets/decoded/sdss/'\n",
    "\n",
    "\n",
    "wlmin, wlmax    = (3388,8318)\n",
    "fixed_num_bins  = 1000\n",
    "min_SN          = 50\n",
    "min_z           = 0.05\n",
    "max_z           = 0.36\n",
    "label           = 'galaxies_quasars_bins%d_wl%d-%d'%(fixed_num_bins,wlmin,wlmax)\n",
    "label_          = label+'_minz%s_maxz%s_minSN%d'%(str(int(min_z*100)).zfill(3),str(int(max_z*100)).zfill(3),min_SN)\n",
    "label_2         = label_+'_10_fully_connected_mean_div'\n",
    "\n",
    "if conditional:\n",
    "    label_2='conditional_%s'%cond_on+label_2\n",
    "\n",
    "upsampling      = 'SMOTE'\n",
    "fac             = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c49c175-f90c-4ffc-b966-19e4254afe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seeds[RUN])\n",
    "np.random.seed(seeds[RUN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655c2b8-475d-4d53-a0dc-7f0e7f7b74ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a4837b7-97d7-45cf-a6b6-798b1475324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train, encoded_valid, encoded_test = np.load(os.path.join(root_encoded,'encoded_%s_RUN%s_new.npy'%(label_2,RUN)), allow_pickle=True)\n",
    "decoded_train,decoded_valid, decoded_test, mean, std = np.load(os.path.join(root_decoded,'decoded_%s_RUN%s_new.npy'%(label_2,RUN)), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e68d26bc-a983-47ca-a390-05bd39350893",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_valid = np.squeeze(encoded_valid)\n",
    "encoded_train = np.squeeze(encoded_train)\n",
    "encoded_test  = np.squeeze(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71e2efe1-2c8b-45e8-982d-d23876ce8462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209462, 69820)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_train), len(encoded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "45c96f50-9810-400a-9b78-f7df404e7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,valid,test,le = pickle.load(open(os.path.join(root_model_data,'combined_%s_new.pkl'%label_),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0449c70a-d2ef-4063-bd57-e496a083f193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'', b'AGN', b'AGN BROADLINE', b'BROADLINE', b'STARBURST',\n",
       "       b'STARBURST BROADLINE', b'STARFORMING', b'STARFORMING BROADLINE'],\n",
       "      dtype='|S32')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(np.arange(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ca6a1-bd00-493a-b70f-2068d0340274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c92dd99-1a0e-4d75-a26c-568ab6a6c53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gal_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1632805-2bcc-418c-9596-a72742307841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16,  27, 106, 430,  10, 703,   6,  43])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sub_class[np.where(gal_type==1)], return_counts=True)[1]#/len(np.where(gal_type==1)[0]),np.unique(sub_class[np.where(gal_type==2)], return_counts=True)[1]/len(np.where(gal_type==2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6448f8ac-9d93-4aa9-aae9-7761b390247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train, test, valid:\n",
    "    sub_class = data['subclass']\n",
    "    gal_type  = data['class']\n",
    "    broadline = []\n",
    "    subclass  = []\n",
    "    gal_class = []\n",
    "    \n",
    "    for ii in range(len(data['subclass'])):\n",
    "        if gal_type[ii]==2:\n",
    "            gal_class.append(0)\n",
    "        elif gal_type[ii]==1:\n",
    "            gal_class.append(1)     \n",
    "        else:\n",
    "            print('unrecognized subclass')\n",
    "        if sub_class[ii] in [0,3]:\n",
    "            subclass.append(0)\n",
    "        elif sub_class[ii] in [1,2]:\n",
    "            subclass.append(1)\n",
    "        elif sub_class[ii] in [4,5]:\n",
    "            subclass.append(2)\n",
    "        elif sub_class[ii] in [6,7]:\n",
    "            subclass.append(3)\n",
    "        else:\n",
    "            print('unrecognized subclass')\n",
    "        if sub_class[ii] in [2,3,5,7]:\n",
    "            broadline.append(1)\n",
    "        elif sub_class[ii] in [0,1,4,6]:\n",
    "            broadline.append(0)\n",
    "        else:\n",
    "            print('unrecognized subclass')    \n",
    "            \n",
    "    broadline = np.asarray(broadline)\n",
    "    subclass  = np.asarray(subclass)\n",
    "    gal_class = np.asarray(gal_class)\n",
    "    \n",
    "    data['labels'] = np.vstack([gal_class,subclass,broadline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ccb8aea8-5b50-4326-9e44-01923c86cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dict0 = {0:'GAL',1: 'QSO'}\n",
    "le_dict1 = {0:'NO SUBCLASS',1: 'AGN', 2:'STARBURST', 3:'STARFORMING'}\n",
    "le_dict2 = {0:'NOT BROADLINE', 1:'BROADLINE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a5dfa473-0d75-4295-ac7c-09d163760373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def names(X, dicts):\n",
    "    N,c = X.shape\n",
    "    assert(len(dicts)==c)\n",
    "    names = []\n",
    "    for ii in range(N):\n",
    "        name = ''\n",
    "        for jj in range(len(dicts)):\n",
    "            if jj==0:\n",
    "                name=''.join([name,dicts[jj][X[ii][jj]]])\n",
    "            else:\n",
    "                #print(dicts[jj][X[ii][jj]])\n",
    "                name=' '.join([name,dicts[jj][X[ii][jj]]])\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5bc97912-f4d7-4a5a-bf23-a62ebb6b211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    N,c = X.shape\n",
    "    print(N,c)\n",
    "    classes = []\n",
    "    counts = []\n",
    "    class_names = []\n",
    "    for ii in range(c):\n",
    "        counts.append(len(np.unique(X[:,ii])))\n",
    "    n_classes = np.prod(counts)\n",
    "    print(n_classes, counts)\n",
    "    for ii in range(N):\n",
    "        class_ = 0\n",
    "        for jj in np.arange((c)):\n",
    "            offset = n_classes/np.prod(counts[0:(jj+1)])\n",
    "            class_+=X[ii][jj]*offset\n",
    "        classes.append(class_)\n",
    "        \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7eb6b247-d311-4024-bbf0-27ac36b26513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209462 3\n",
      "16 [2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "res   = flatten(train['labels'].T)\n",
    "names = names(train['labels'].T,[le_dict0,le_dict1,le_dict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "75616532-0de1-474e-b6fa-6e352eb6a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['names']  = names\n",
    "df['labels'] = res\n",
    "df = df[['names', 'labels']].drop_duplicates().reset_index().drop(columns=['index'])\n",
    "\n",
    "le_new = pd.Series(df.names.values,index=df.labels).to_dict()\n",
    "pickle.dump(le_new, open('new_inflated_labels.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "12cf10d3-88be-4fbd-bcfb-3519422033d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209462 3\n",
      "16 [2, 4, 2]\n",
      "69820 3\n",
      "16 [2, 4, 2]\n",
      "69822 3\n",
      "16 [2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "for data in [train,valid, test]:\n",
    "    data['new_inf_labels'] = flatten(data['labels'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42b895-9319-4c7f-bcc3-3ae32057d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler = Samplers[upsampling]\n",
    "# unique, counts = np.unique(train['subclass'], return_counts=True)\n",
    "# min_counts = np.amin(counts)\n",
    "# print(min_counts)\n",
    "# sampling_strategy = {0: max(counts[0],min_counts*fac), 1:max(counts[1],min_counts*fac), 2: max(counts[2],min_counts*fac), 3: max(counts[3],min_counts*fac), 4:max(counts[4],min_counts*fac)\\\n",
    "#                      , 5:max(counts[5],min_counts*fac), 6:max(counts[6],min_counts*fac), 7:max(counts[7],min_counts*fac)}\n",
    "# smt = Sampler(sampling_strategy=sampling_strategy)\n",
    "# X_smt, y_smt = smt.fit_resample(encoded_train,train['subclass'])\n",
    "\n",
    "# unique, counts = np.unique(valid['subclass'], return_counts=True)\n",
    "# min_counts = np.amin(counts)\n",
    "# print(min_counts)\n",
    "# sampling_strategy = {0: max(counts[0],min_counts*fac), 1:max(counts[1],min_counts*fac), 2: max(counts[2],min_counts*fac), 3: max(counts[3],min_counts*fac), 4:max(counts[4],min_counts*fac)\\\n",
    "#                      , 5:max(counts[5],min_counts*fac), 6:max(counts[6],min_counts*fac), 7:max(counts[7],min_counts*fac)}\n",
    "# smt = Sampler(sampling_strategy=sampling_strategy)\n",
    "# X_smt_v, y_smt_v = smt.fit_resample(encoded_valid,valid['subclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d65eaa-2516-4a23-a712-048370813fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "  array([128846,   4647,   1980,   3384,  11885,   2033,  57467,   1980])),\n",
       " (array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "  array([42870,  1576,   660,  1115,  4050,   711, 19065,   660])))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(y_smt, return_counts=True), np.unique(y_smt_v, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "de8d0d68-e039-455a-9906-39b49b77ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(encoded_train).to(device) \n",
    "train_y = torch.Tensor(train['new_inf_labels']).to(torch.long).to(device)\n",
    "\n",
    "valid_x = torch.Tensor(encoded_valid).to(device) \n",
    "valid_y = torch.Tensor(valid['new_inf_labels']).to(torch.long).to(device)\n",
    "\n",
    "test_x = torch.Tensor(encoded_test).to(device) \n",
    "test_y = torch.Tensor(test['new_inf_labels']).to(torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b9091bf3-01c4-45af-9cb5-3ad3ec192de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conditional_transform_batch_model(model, data, label, logj, index, batchsize, start_index=0, end_index=None, start=0, end=None, param=None, nocuda=False):\n",
    "\n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        gpu    = index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:%d'%gpu)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    i = 0\n",
    "    while i * batchsize < end_index-start_index:\n",
    "        start_index0 = start_index + i * batchsize \n",
    "        end_index0 = min(start_index + (i+1) * batchsize, end_index) \n",
    "        if param is None:\n",
    "            data1, logj1 = model.transform(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), start=start, end=end, param=param)\n",
    "        else:\n",
    "            data1, logj1 = model.transform(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), start=start, end=end, param=param[start_index0:end_index0].to(device))\n",
    "        data[start_index0:end_index0] = data1.to(data.device)\n",
    "        logj[start_index0:end_index0] = logj[start_index0:end_index0] + logj1.to(logj.device)\n",
    "        i += 1\n",
    "\n",
    "    del data1, logj1, model \n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def conditional_transform_batch_model(model, data, label, batchsize, logj=None, start=0, end=None, param=None, pool=None, nocuda=False):\n",
    "\n",
    "    if logj is None:\n",
    "        logj = torch.zeros(len(data), device=data.device)\n",
    "\n",
    "    if pool is None: \n",
    "        _transform_batch_model(model, data, label, logj, 0, batchsize, start=start, end=end, param=param, nocuda=nocuda) \n",
    "    else:\n",
    "        if torch.cuda.is_available() and not nocuda:\n",
    "            nprocess = torch.cuda.device_count()\n",
    "        else:\n",
    "            nprocess = mp.cpu_count()\n",
    "        param0 = [(model, data, label, logj, i, batchsize, len(data)*i//nprocess, len(data)*(i+1)//nprocess, start, end, param, nocuda) for i in range(nprocess)]\n",
    "        pool.starmap(_conditional_transform_batch_model, param0)\n",
    "\n",
    "    return data, logj\n",
    "\n",
    "\n",
    "def _conditional_transform_batch_layer(layer, data, label, logj, index, batchsize, start_index=0, end_index=None, direction='forward', param=None, nocuda=False):\n",
    "\n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        gpu = index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:%d'%gpu)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    layer = layer.to(device)\n",
    "\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    i = 0\n",
    "    while i * batchsize < end_index-start_index:\n",
    "        start_index0 = start_index + i * batchsize \n",
    "        end_index0 = min(start_index + (i+1) * batchsize, end_index) \n",
    "        if direction == 'forward': \n",
    "            if param is None:\n",
    "                data1, logj1 = layer.forward(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param)\n",
    "            else:\n",
    "                data1, logj1 = layer.forward(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), param=param[start_index0:end_index0].to(device))\n",
    "        else: \n",
    "            if param is None:\n",
    "                data1, logj1 = layer.inverse(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param)\n",
    "            else:\n",
    "                data1, logj1 = layer.inverse(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param[start_index0:end_index0].to(device))\n",
    "        data[start_index0:end_index0] = data1.to(data.device)\n",
    "        logj[start_index0:end_index0] = logj[start_index0:end_index0] + logj1.to(logj.device)\n",
    "        i += 1\n",
    "\n",
    "    del data1, logj1, layer \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def conditional_transform_batch_layer(layer, data, label, batchsize, logj=None, direction='forward', param=None, pool=None, nocuda=False):\n",
    "    assert direction in ['forward', 'inverse']\n",
    "    \n",
    "    if logj is None:\n",
    "        logj = torch.zeros(len(data), device=data.device)\n",
    "    \n",
    "    if pool is None: \n",
    "        _conditional_transform_batch_layer(layer, data, label, logj, 0, batchsize, direction=direction, param=param, nocuda=nocuda) \n",
    "    else:\n",
    "        if torch.cuda.is_available() and not nocuda:\n",
    "            nprocess = torch.cuda.device_count()\n",
    "        else:\n",
    "            nprocess = mp.cpu_count()\n",
    "        param0 = [(layer, data, label, logj, i, batchsize, len(data)*i//nprocess, len(data)*(i+1)//nprocess, direction, param, nocuda) for i in range(nprocess)]\n",
    "        pool.starmap(_conditional_transform_batch_layer, param0)\n",
    "    \n",
    "    return data, logj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3605197d-43e8-4fe5-9aec-9f81620575ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSINF(nn.Module):\n",
    "\n",
    "    #Sliced Iterative Normalizing Flow model\n",
    "    \n",
    "    def __init__(self, ndim, n_class):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.ModuleList([])\n",
    "        self.ndim = ndim\n",
    "        self.n_class = n_class\n",
    "    \n",
    "    def forward(self, data, label, start=0, end=None):\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data.view(1,-1)\n",
    "        if end is None:\n",
    "            end = len(self.layer)\n",
    "        elif end < 0:\n",
    "            end += len(self.layer)\n",
    "        if start < 0:\n",
    "            start += len(self.layer)\n",
    "        \n",
    "        assert start >= 0 and end >= 0 and end >= start\n",
    "\n",
    "        logj = torch.zeros(data.shape[0], device=data.device)\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            data, log_j = self.layer[i](data, param=label)\n",
    "            logj += log_j\n",
    "\n",
    "        return data, logj\n",
    "    \n",
    "    \n",
    "    def inverse(self, data, label, start=None, end=0, d_dz=None):\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data.view(1,-1)\n",
    "        if end < 0:\n",
    "            end += len(self.layer)\n",
    "        if start is None:\n",
    "            start = len(self.layer)\n",
    "        elif start < 0:\n",
    "            start += len(self.layer)\n",
    "        \n",
    "        assert start >= 0 and end >= 0 and end <= start\n",
    "\n",
    "        logj = torch.zeros(data.shape[0], device=data.device)\n",
    "        \n",
    "        for i in reversed(range(end, start)):\n",
    "            if d_dz is None:\n",
    "                data, log_j = self.layer[i].inverse(data, param=label)\n",
    "            else:\n",
    "                data, log_j, d_dz = self.layer[i].inverse(data,d_dz=d_dz, param=label)\n",
    "            logj += log_j\n",
    "\n",
    "        if d_dz is None:\n",
    "            return data, logj\n",
    "        else:\n",
    "            return data, logj, d_dz\n",
    "\n",
    "\n",
    "    def transform(self, data, label, start, end,):\n",
    "\n",
    "        if start is None:\n",
    "            return self.inverse(data=data, start=start, end=end, param=label) \n",
    "        elif end is None:\n",
    "            return self.forward(data=data, start=start, end=end, param=label) \n",
    "        elif start < 0:\n",
    "            start += len(self.layer)\n",
    "        elif end < 0:\n",
    "            end += len(self.layer)\n",
    "        \n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        elif start > len(self.layer):\n",
    "            start = len(self.layer)\n",
    "        if end < 0:\n",
    "            end = 0\n",
    "        elif end > len(self.layer):\n",
    "            end = len(self.layer)\n",
    "\n",
    "        if start <= end:\n",
    "            return self.forward(data=data, start=start, end=end, param=label) \n",
    "        else:\n",
    "            return self.inverse(data=data, start=start, end=end, param=label) \n",
    "    \n",
    "    \n",
    "    def add_layer(self, layer, position=None):\n",
    "        \n",
    "        if position is None or position == len(self.layer):\n",
    "            self.layer.append(layer)\n",
    "        else:\n",
    "            if position < 0:\n",
    "                position += len(self.layer)\n",
    "            assert position >= 0 and position < len(self.layer)\n",
    "            self.layer.insert(position, layer)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def delete_layer(self, position=-1):\n",
    "        \n",
    "        if position == -1 or position == len(self.layer)-1:\n",
    "            self.layer = self.layer[:-1]\n",
    "        else:\n",
    "            if position < 0:\n",
    "                position += len(self.layer)\n",
    "            assert position >= 0 and position < len(self.layer)-1\n",
    "            \n",
    "            for i in range(position, len(self.layer)-1):\n",
    "                self.layer._modules[str(i)] = self.layer._modules[str(i + 1)]\n",
    "            self.layer = self.layer[:-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def evaluate_density(self, data, label, start=0, end=None):\n",
    "        \n",
    "        data, logj = self.forward(data, label, start=start, end=end)\n",
    "        logq = -self.ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.sum(data.reshape(len(data), self.ndim)**2,  dim=1)/2\n",
    "        logp = logj + logq\n",
    "        \n",
    "        return logp\n",
    "\n",
    "\n",
    "    def loss(self, data, start=0, end=None, param=None):\n",
    "        return -torch.mean(self.evaluate_density(data, label, start=start, end=end, param=param))\n",
    "    \n",
    "    \n",
    "    def sample(self, nsample, start=None, end=0, device=torch.device('cuda'), param=None):\n",
    "\n",
    "        #device must be the same as the device of the model\n",
    "        \n",
    "        x       = torch.randn(nsample, self.ndim, device=device)\n",
    "        logq    = -self.ndim/2.*torch.log(torch.tensor(2.*math.pi)) - torch.sum(x**2,  dim=1)/2\n",
    "        x, logj = self.inverse(x, start=start, end=end, param=param)\n",
    "        logp    = logj + logq\n",
    "\n",
    "        return x, logp\n",
    "\n",
    "\n",
    "    def score(self, data, label, start=0, end=None, param=None):\n",
    "\n",
    "        #returns score = dlogp / dx\n",
    "\n",
    "        data.requires_grad_(True)\n",
    "        logp  = torch.sum(self.evaluate_density(data, label, start, end, param))\n",
    "        score = torch.autograd.grad(logp, data)[0]\n",
    "        data.requires_grad_(False)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a1fbb52d-228c-454d-8703-8de6301f29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ConditionalGIS(data_train, label_train, data_valid, label_valid, iteration=None, K=None, M=None, KDE=True, b_factor=1, alpha=None, bounds=None,max_iter=400,\n",
    "        edge_bins=None, ndata_A=None, MSWD_max_iter=None, NBfirstlayer=False, Whiten=True, batchsize=None, nocuda=False, patch=False, shape=None, model=None, verbose=True):\n",
    "    \n",
    "    '''\n",
    "    data_train: (ndata_train, ndim).\n",
    "    data_valid: (ndata_valid, ndim), optional. If provided, its logp will be used to determine the number of iterations.\n",
    "    iteration: integer, optional. The maximum number of GIS iterations. Required if data_valid is not provided.\n",
    "    K: integer, optional. The number of slices for each iteration. See max K-SWD in the SINF paper. 1 <= K <= ndim.\n",
    "    M: integer, optional. The number of spline knots for rational quadratic splines.\n",
    "    KDE: bool. Whether to use KDE for estimating 1D PDF. Recommended True.\n",
    "    b_factor: positive float number, optional. The multiplicative factor for KDE kernel width.\n",
    "    alpha: two non-negative float number in the format of (alpha1, alpha2), optional. Regularization parameter. See Equation 13 of SINF paper. alpha1 for interpolation, alpha2 for extrapolation slope. 0 <= alpha1,2 < 1. If not given, very heavy regularization will be used, which could result in slow training and a large number of iterations.\n",
    "    bounds: sequence, optional. In the format of [[x1_min, x1_max], [x2_min, x2_max], ..., [xd_min, xd_max]]. Represent infinity and negative infinity with None.\n",
    "    edge_bins: non-negative integer, optional. The number of spline knots at the boundary.\n",
    "    ndata_A: positive integer, optional. The number of training data used for fitting A (slice axes).\n",
    "    MSWD_max_iter: positive integer, optional. The maximum number of iterations for optimizing A (slice axes). See Algorithm 1 of SINF paper. Called L_iter in the paper.\n",
    "    NBfirstlayer: bool, optional. Whether to use Naive Bayes (no rotation) at the first layer.\n",
    "    Whiten: bool, optional. Whether to whiten the data before applying GIS.\n",
    "    batchsize: positive integer, optional. The batch size for transforming the data. Does not change the performance. Only saves the memory. \n",
    "    Useful when the data is too large and can't fit in the memory.\n",
    "    nocuda: bool, optional. Whether to use gpu.\n",
    "    patch: bool, optional. Whether to use patch-based modeling. Only useful for image datasets.\n",
    "    shape: sequence, optional. The shape of the image datasets, if patch is enabled.\n",
    "    model: GIS model, optional. Trained GIS model. If provided, new iterations will be added in the model.\n",
    "    verbose: bool, optional. Whether to print training information.\n",
    "    '''\n",
    "\n",
    "    assert data_valid is not None or iteration is not None\n",
    " \n",
    "    #hyperparameters\n",
    "    ndim    = data_train.shape[1]\n",
    "    nclass  = len(np.unique(label_train.cpu().numpy()))\n",
    "    print(ndim,nclass)\n",
    "\n",
    "    ndata = len(data_train)\n",
    "\n",
    "    if M is None:\n",
    "        M = max(min(200, int(ndata**0.5)), 50)\n",
    "    if alpha is None:\n",
    "        alpha = (1-0.02*math.log10(ndata), 1-0.001*math.log10(ndata))#).to(device)\n",
    "    if bounds is not None:\n",
    "        assert len(bounds) == ndim\n",
    "        for i in range(ndim):\n",
    "            assert len(bounds[i]) == 2\n",
    "    if edge_bins is None:\n",
    "        edge_bins = max(int(math.log10(ndata))-1, 0)\n",
    "    if batchsize is None:\n",
    "        batchsize = len(data_train)\n",
    "    if not patch:\n",
    "        if K is None:\n",
    "            if ndim <= 8 or ndata / float(ndim) < 20:\n",
    "                K = ndim\n",
    "            else:\n",
    "                K = 8\n",
    "        if ndata_A is None:\n",
    "            ndata_A = min(len(data_train), int(math.log10(ndim)*1e5))\n",
    "        if MSWD_max_iter is None:\n",
    "            MSWD_max_iter = min(round(ndata) // ndim, 200)\n",
    "    else:\n",
    "        assert shape[0] > 4 and shape[1] > 4\n",
    "        K0 = K\n",
    "        ndata_A0 = ndata_A\n",
    "        MSWD_max_iter0 = MSWD_max_iter\n",
    "\n",
    "        \n",
    "    best_accuracy = 0\n",
    "    #device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and not nocuda else \"cpu\")\n",
    "\n",
    "    \n",
    "    ### taking out logit transform and replacing log jacobian with zeros \n",
    "    logj_train    = torch.zeros(data_train.shape[0],device=device)\n",
    "    logj_valid    = torch.zeros(data_valid.shape[0],device=device)\n",
    "\n",
    "    #define the model\n",
    "    if model is None:\n",
    "        model = ConditionalSINF(ndim=ndim,n_class=nclass).requires_grad_(False).to(device)\n",
    "        if data_valid is not None:\n",
    "            best_logp_valid = -1e10\n",
    "            best_Nlayer     = 0\n",
    "            wait            = 0\n",
    "            maxwait         = 5 \n",
    "#     else:\n",
    "#         t = time.time()\n",
    "#         data_train, logj_train = conditional_transform_batch_model(model, data_train, batchsize, logj=logj_train, start=0, end=None, nocuda=nocuda)\n",
    "#         logp_train             = (torch.mean(logj_train) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train**2,  dim=1)/2)).item()\n",
    "        \n",
    "#         if data_valid is not None:\n",
    "#             data_valid, logj_valid = conditional_transform_batch_model(model, data_valid, batchsize, logj=logj_valid, start=0, end=None, nocuda=nocuda)\n",
    "#             logp_valid             = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "#             print ('Initial logp:', logp_train, logp_valid, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "#         else:\n",
    "#             print ('Initial logp:', logp_train, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "\n",
    "    \n",
    "    #whiten\n",
    "    if Whiten:\n",
    "        layer = whiten(ndim_data=ndim, scale=True, ndim_latent=ndim).requires_grad_(False).to(device)\n",
    "        layer.fit(data_train)\n",
    "\n",
    "        data_train, logj_train0 = layer(data_train)\n",
    "        logj_train += logj_train0\n",
    "\n",
    "        logp_train = (torch.mean(logj_train) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train**2,  dim=1)/2)).item()\n",
    "        \n",
    "        if data_valid is not None:\n",
    "            data_valid, logj_valid0 = layer(data_valid)\n",
    "            logj_valid += logj_valid0\n",
    "            logp_valid = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "            if logp_valid > best_logp_valid:\n",
    "                best_logp_valid = logp_valid\n",
    "                best_Nlayer = len(model.layer)\n",
    "\n",
    "        model.add_layer(layer)\n",
    "        if verbose:\n",
    "            if data_valid is not None:\n",
    "                print('After whiten logp:', logp_train, logp_valid)\n",
    "            else:\n",
    "                print('After whiten logp:', logp_train)\n",
    "\n",
    "                \n",
    "                \n",
    "    data_train    = data_train[None,:].repeat_interleave(nclass, axis=0)\n",
    "    data_valid    = data_valid[None,:].repeat_interleave(nclass, axis=0)\n",
    "\n",
    "    logj_train    = logj_train[None,:].repeat_interleave(nclass, axis=0)\n",
    "    logj_valid    = logj_valid[None,:].repeat_interleave(nclass, axis=0)\n",
    "\n",
    "    #GIS iterations\n",
    "    \n",
    "    ii=0\n",
    "    while True:\n",
    "        t = time.time()\n",
    "\n",
    "        if NBfirstlayer:\n",
    "            layer = ConditionalSlicedTransport_discrete(ndim=ndim, n_class = nclass, K=ndim, M=M).requires_grad_(False).to(device)\n",
    "        else:\n",
    "            layer = ConditionalSlicedTransport_discrete(ndim=ndim, n_class = nclass, K=K, M=M).requires_grad_(False).to(device)\n",
    "\n",
    "        #fit the layer\n",
    "        if NBfirstlayer:\n",
    "            layer.A[:] = torch.eye(ndim).to(device)\n",
    "            NBfirstlayer = False\n",
    "        elif ndim > 1:\n",
    "            layer.fit_A(data_train[label_train, torch.arange(data_train.shape[1]).cuda()], MSWD_max_iter=MSWD_max_iter, verbose=verbose)\n",
    "\n",
    "        layer.fit_spline(data_train[label_train, torch.arange(data_train.shape[1]).cuda()], label_train, edge_bins=edge_bins, derivclip=1, alpha=alpha, KDE=KDE, b_factor=b_factor, verbose=False)\n",
    "        \n",
    "        for label in range(nclass):\n",
    "            data_train[label], logj_train1 = layer(data_train[label], torch.ones(data_train.shape[1], dtype=torch.int, device=data_train.device)*label)\n",
    "            logj_train[label] = logj_train[label] + logj_train1\n",
    "\n",
    "            data_valid[label], logj_valid1 = layer(data_valid[label], torch.ones(data_valid.shape[1], dtype=torch.int, device=data_valid.device)*label)\n",
    "            logj_valid[label] = logj_valid[label] + logj_valid1\n",
    "\n",
    "        model.add_layer(layer)\n",
    "\n",
    "\n",
    "#     for label in range(n_class):\n",
    "#         data_train[label], logj_train1 = layer(data_train[label], torch.ones(data_train.shape[1], dtype=torch.int, device=data_train.device)*label)\n",
    "#         logp_train[label]              = logj_train - torch.sum(data_train**2,  dim=1)/2- ndim/2*torch.log(torch.tensor(2*math.pi))\n",
    "\n",
    "#         data_valid[label], logj_valid1 = layer(data_validate[label], torch.ones(data_validate.shape[1], dtype=torch.int, device=data_validate.device)*label)\n",
    "#         logp_valid[label]              = logj_valid\n",
    "\n",
    "#         data_test[label], logj_test1 = layer(data_test[label], torch.ones(data_test.shape[1], dtype=torch.int, device=data_test.device)*label)\n",
    "#         logj_test[label]             = logj_test[label] + logj_test1\n",
    "        \n",
    "        logp_train = (torch.mean(logj_train[label_train, torch.arange(data_train.shape[1]).cuda()]) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train[label_train, torch.arange(data_train.shape[1]).cuda()]**2,  dim=1)/2)).item()\n",
    "        logp_valid = (torch.mean(logj_valid[label_valid, torch.arange(data_valid.shape[1]).cuda()]) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid[label_valid, torch.arange(data_valid.shape[1]).cuda()]**2,  dim=1)/2)).item()\n",
    "\n",
    "        \n",
    "#         logp_valid_label = torch.ones([nclass,label_valid.shape[0]], device=data_valid.device)\n",
    "#         if data_valid is not None:\n",
    "#             for mm in range(nclass):\n",
    "#                 data_valid, logj_valid = conditional_transform_batch_layer(layer, data_valid, torch.ones(label_valid.shape, dtype=torch.int, device=data_valid.device)*mm, batchsize, logj=logj_valid, direction='forward', nocuda=nocuda)\n",
    "#                 logp_valid = (logj_valid - torch.sum(data_valid**2,  dim=1)/2)- ndim/2*torch.log(torch.tensor(2*math.pi))\n",
    "#                 logp_valid_label[mm] = logp_valid\n",
    "#             predict_label_valid = torch.argmax(logp_valid_label,dim=0)\n",
    "#             accuracy = torch.sum(predict_label_valid==label_valid).item() / len(label_valid)\n",
    "#             print('valid accuracy: ', torch.sum(predict_label_valid==label_valid).item() / len(label_valid))\n",
    "    \n",
    "#             data_valid, logj_valid = conditional_transform_batch_layer(layer, data_valid, label_valid, batchsize, logj=logj_valid, direction='forward', nocuda=nocuda)\n",
    "#             logp_valid = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "\n",
    "            \n",
    "        if logp_valid > best_logp_valid:\n",
    "            best_logp_valid = logp_valid\n",
    "            best_Nlayer = len(model.layer)\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait == maxwait:\n",
    "            model.layer = model.layer[:best_Nlayer]\n",
    "            break\n",
    "        if ii>max_iter:\n",
    "            break\n",
    "\n",
    "#         if  accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_Nlayer_acc = len(model.layer)\n",
    "#             wait_acc = 0\n",
    "#         else:\n",
    "#             wait_acc += 1\n",
    "#         if wait_acc == maxwait:\n",
    "#             model.layer = model.layer[:best_Nlayer_acc]\n",
    "#             break\n",
    "\n",
    "        if verbose:\n",
    "            if data_valid is not None: \n",
    "                print ('logp:', logp_train, logp_valid, 'time:', time.time()-t, 'iteration:', len(model.layer), 'best:', best_Nlayer)\n",
    "            else:\n",
    "                print ('logp:', logp_train, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "\n",
    "        if iteration is not None and len(model.layer) >= iteration:\n",
    "            if data_valid is not None:\n",
    "                model.layer = model.layer[:best_Nlayer]\n",
    "            break\n",
    "        ii+=1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d9c00-426e-4477-b47a-437dcfdd12b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 16\n",
      "After whiten logp: -16.6646671295166 -15.997014999389648\n",
      "Fit A: Time: 2.57357666015625 Wasserstein Distance: [1.2356852293014526, 1.0577586889266968, 1.0078697204589844, 0.9537957906723022, 0.8020996451377869, 0.7809862494468689, 0.7651858925819397, 0.731254518032074]\n",
      "logp: -13.651651382446289 -13.07955551147461 time: 7.4064106941223145 iteration: 2 best: 2\n",
      "Fit A: Time: 1.346484619140625 Wasserstein Distance: [1.118345856666565, 0.9617394208908081, 0.8923488855361938, 0.8692584037780762, 0.8204649090766907, 0.7469224333763123, 0.6785220503807068, 0.575510561466217]\n",
      "logp: -11.863985061645508 -11.370375633239746 time: 5.188464641571045 iteration: 3 best: 3\n",
      "Fit A: Time: 1.3833079833984374 Wasserstein Distance: [0.9587300419807434, 0.9403561353683472, 0.8172072768211365, 0.807624876499176, 0.7194579839706421, 0.7074853181838989, 0.6025698781013489, 0.5352482795715332]\n",
      "logp: -10.540392875671387 -10.111053466796875 time: 4.8538243770599365 iteration: 4 best: 4\n",
      "Fit A: Time: 1.38333837890625 Wasserstein Distance: [0.9312151074409485, 0.8017494082450867, 0.7505743503570557, 0.7335290312767029, 0.7197438478469849, 0.6125204563140869, 0.5272645354270935, 0.5123224258422852]\n",
      "logp: -9.377950668334961 -9.004302978515625 time: 4.875296592712402 iteration: 5 best: 5\n",
      "Fit A: Time: 0.88373876953125 Wasserstein Distance: [0.8154457807540894, 0.7791778445243835, 0.7201250195503235, 0.6336241364479065, 0.6289864182472229, 0.6027556657791138, 0.5124233961105347, 0.46671801805496216]\n",
      "logp: -8.45680046081543 -8.12728500366211 time: 3.839891195297241 iteration: 6 best: 6\n",
      "Fit A: Time: 0.89502783203125 Wasserstein Distance: [0.7598609328269958, 0.6746464967727661, 0.6652791500091553, 0.5948914289474487, 0.5837846398353577, 0.5239166617393494, 0.5235446691513062, 0.5007973313331604]\n",
      "logp: -7.763907432556152 -7.469094276428223 time: 3.5365469455718994 iteration: 7 best: 7\n",
      "Fit A: Time: 1.2551671142578125 Wasserstein Distance: [0.6784826517105103, 0.6131638288497925, 0.5788137316703796, 0.5597236752510071, 0.5543866753578186, 0.5543133020401001, 0.5017148852348328, 0.47947901487350464]\n",
      "logp: -7.15985631942749 -6.894075870513916 time: 4.3097615242004395 iteration: 8 best: 8\n",
      "Fit A: Time: 1.3698408203125 Wasserstein Distance: [0.656592845916748, 0.5838005542755127, 0.5605306029319763, 0.5421357154846191, 0.4866098463535309, 0.4633936583995819, 0.45119497179985046, 0.4460938572883606]\n",
      "logp: -6.609539985656738 -6.364957809448242 time: 4.7828264236450195 iteration: 9 best: 9\n",
      "Fit A: Time: 1.391738037109375 Wasserstein Distance: [0.6252769827842712, 0.5361628532409668, 0.5063895583152771, 0.5002644062042236, 0.49511227011680603, 0.44857385754585266, 0.4243278205394745, 0.40802502632141113]\n",
      "logp: -6.160619735717773 -5.937033176422119 time: 4.810408353805542 iteration: 10 best: 10\n",
      "Fit A: Time: 1.3830609130859375 Wasserstein Distance: [0.5447203516960144, 0.5297200679779053, 0.4877108633518219, 0.4817310869693756, 0.4659605026245117, 0.431286484003067, 0.38488340377807617, 0.3792133927345276]\n",
      "logp: -5.787403583526611 -5.581904411315918 time: 4.761972904205322 iteration: 11 best: 11\n",
      "Fit A: Time: 1.3937401123046875 Wasserstein Distance: [0.532467246055603, 0.5088660717010498, 0.45886504650115967, 0.45630839467048645, 0.40761974453926086, 0.40174731612205505, 0.3827415406703949, 0.36353257298469543]\n",
      "logp: -5.437722682952881 -5.248423099517822 time: 4.805122137069702 iteration: 12 best: 12\n",
      "Fit A: Time: 1.397612060546875 Wasserstein Distance: [0.48355287313461304, 0.46147269010543823, 0.46079006791114807, 0.4115463197231293, 0.39509597420692444, 0.381857693195343, 0.3815794289112091, 0.3586195111274719]\n",
      "logp: -5.132619380950928 -4.957126617431641 time: 4.837034225463867 iteration: 13 best: 13\n",
      "Fit A: Time: 1.0516256103515624 Wasserstein Distance: [0.5045027732849121, 0.4439731538295746, 0.4221611022949219, 0.383289098739624, 0.37747251987457275, 0.35051941871643066, 0.3325500190258026, 0.32692474126815796]\n",
      "logp: -4.877915859222412 -4.711641311645508 time: 3.980121612548828 iteration: 14 best: 14\n",
      "Fit A: Time: 1.344193359375 Wasserstein Distance: [0.473285973072052, 0.40132981538772583, 0.3895361125469208, 0.38504332304000854, 0.3790132701396942, 0.34269922971725464, 0.3345237970352173, 0.31382328271865845]\n",
      "logp: -4.642975330352783 -4.48682165145874 time: 3.9567837715148926 iteration: 15 best: 15\n",
      "Fit A: Time: 1.3681124267578124 Wasserstein Distance: [0.47443652153015137, 0.4621579647064209, 0.3659448027610779, 0.3356950879096985, 0.3284410238265991, 0.3043507933616638, 0.30374881625175476, 0.3018474280834198]\n",
      "logp: -4.451343536376953 -4.303685188293457 time: 4.767440557479858 iteration: 16 best: 16\n",
      "Fit A: Time: 1.379245849609375 Wasserstein Distance: [0.43413442373275757, 0.42271578311920166, 0.34998270869255066, 0.34758520126342773, 0.32895439863204956, 0.29759731888771057, 0.2951648235321045, 0.28621429204940796]\n",
      "logp: -4.28658390045166 -4.146811485290527 time: 6.062601566314697 iteration: 17 best: 17\n",
      "Fit A: Time: 1.273073974609375 Wasserstein Distance: [0.43392831087112427, 0.40247687697410583, 0.3328099250793457, 0.3123677670955658, 0.30702394247055054, 0.30043262243270874, 0.2974357306957245, 0.27883410453796387]\n",
      "logp: -4.124117851257324 -3.9918761253356934 time: 7.531275272369385 iteration: 18 best: 18\n",
      "Fit A: Time: 0.7690371704101563 Wasserstein Distance: [0.41496264934539795, 0.36176708340644836, 0.33727964758872986, 0.3151109516620636, 0.30913472175598145, 0.2869347035884857, 0.2866230309009552, 0.2807703912258148]\n",
      "logp: -3.9756979942321777 -3.8488378524780273 time: 5.402288436889648 iteration: 19 best: 19\n",
      "Fit A: Time: 1.406697021484375 Wasserstein Distance: [0.40701723098754883, 0.37632760405540466, 0.3716332018375397, 0.30328449606895447, 0.2703459560871124, 0.24773918092250824, 0.24185962975025177, 0.24164816737174988]\n",
      "logp: -3.847024917602539 -3.7247743606567383 time: 6.096715927124023 iteration: 20 best: 20\n",
      "Fit A: Time: 1.4361263427734374 Wasserstein Distance: [0.4634324312210083, 0.4351050853729248, 0.26079070568084717, 0.25158581137657166, 0.23803329467773438, 0.22999578714370728, 0.2292269915342331, 0.2244986742734909]\n",
      "logp: -3.7382588386535645 -3.620119094848633 time: 7.3299243450164795 iteration: 21 best: 21\n",
      "Fit A: Time: 1.12606494140625 Wasserstein Distance: [0.4222954213619232, 0.4102834165096283, 0.2886274755001068, 0.24531713128089905, 0.24422159790992737, 0.23761217296123505, 0.23616789281368256, 0.2206563502550125]\n",
      "logp: -3.6459436416625977 -3.5323386192321777 time: 4.568573474884033 iteration: 22 best: 22\n",
      "Fit A: Time: 1.37765576171875 Wasserstein Distance: [0.4142133295536041, 0.32837721705436707, 0.31761083006858826, 0.3083665072917938, 0.2419978231191635, 0.23029941320419312, 0.21821507811546326, 0.20868058502674103]\n",
      "logp: -3.5511035919189453 -3.4417195320129395 time: 4.812667608261108 iteration: 23 best: 23\n",
      "Fit A: Time: 0.88034375 Wasserstein Distance: [0.4096836447715759, 0.3634321391582489, 0.31524962186813354, 0.2532830238342285, 0.2382097840309143, 0.20970124006271362, 0.2096674144268036, 0.20502763986587524]\n",
      "logp: -3.4628071784973145 -3.35689115524292 time: 4.3257904052734375 iteration: 24 best: 24\n",
      "Fit A: Time: 1.3943968505859374 Wasserstein Distance: [0.4098915457725525, 0.3566298186779022, 0.2896236479282379, 0.2727925181388855, 0.21873295307159424, 0.2079915702342987, 0.19785402715206146, 0.19735415279865265]\n",
      "logp: -3.385925769805908 -3.2830686569213867 time: 4.762951374053955 iteration: 25 best: 25\n",
      "Fit A: Time: 1.3859456787109374 Wasserstein Distance: [0.40076836943626404, 0.3993667960166931, 0.2804063558578491, 0.20890064537525177, 0.20150941610336304, 0.19426432251930237, 0.19070519506931305, 0.18626250326633453]\n",
      "logp: -3.3201937675476074 -3.220195770263672 time: 4.7803754806518555 iteration: 26 best: 26\n",
      "Fit A: Time: 1.384533935546875 Wasserstein Distance: [0.43301478028297424, 0.41500234603881836, 0.19390349090099335, 0.19294755160808563, 0.18894915282726288, 0.18869802355766296, 0.1871175765991211, 0.18181292712688446]\n",
      "logp: -3.2585225105285645 -3.161240577697754 time: 4.786777019500732 iteration: 27 best: 27\n",
      "Fit A: Time: 0.8714548950195312 Wasserstein Distance: [0.34643182158470154, 0.29275768995285034, 0.28922203183174133, 0.2755744457244873, 0.2614317834377289, 0.24013717472553253, 0.19231374561786652, 0.17410121858119965]\n",
      "logp: -3.190439224243164 -3.096273422241211 time: 4.262600421905518 iteration: 28 best: 28\n",
      "Fit A: Time: 1.374086181640625 Wasserstein Distance: [0.4202544391155243, 0.3224290609359741, 0.2525121867656708, 0.23865114152431488, 0.19804909825325012, 0.19660919904708862, 0.1883922964334488, 0.16741280257701874]\n"
     ]
    }
   ],
   "source": [
    "model = train_ConditionalGIS(train_x, train_y,valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "021f0c6e-df77-44db-9a47-bd33b23e92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,os.path.join(root_models,'conditional_SINF_%s_%s_%d_AE1'%(label_2,upsampling,fac)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
