{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VMBoehm/SDSS_PAE/blob/main/LSTM_AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqDqXXTQ1FMN"
   },
   "source": [
    "# Template and tests for an LSTM Auto-Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZcR4zT3V8dMc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///global/homes/v/vboehm/codes/SDSS_PAE\n",
      "Requirement already satisfied: astropy in /global/u2/v/vboehm/.local/lib/python3.8/site-packages (from sdss-pae==0.1.0) (4.2.1)\n",
      "Requirement already satisfied: numpy in /global/common/cori_cle7/software/jupyter/cgpu/21-03/lib/python3.8/site-packages (from sdss-pae==0.1.0) (1.20.2)\n",
      "Requirement already satisfied: pyerfa in /global/u2/v/vboehm/.local/lib/python3.8/site-packages (from astropy->sdss-pae==0.1.0) (1.7.3)\n",
      "Installing collected packages: sdss-pae\n",
      "  Attempting uninstall: sdss-pae\n",
      "    Found existing installation: sdss-pae 0.1.0\n",
      "    Uninstalling sdss-pae-0.1.0:\n",
      "      Successfully uninstalled sdss-pae-0.1.0\n",
      "  Running setup.py develop for sdss-pae\n",
      "Successfully installed sdss-pae\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "! pip install --user -e /global/homes/v/vboehm/codes/SDSS_PAE/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Layer, Reshape, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, UpSampling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import RepeatVector, Conv2DTranspose, Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed, Input, Lambda, Masking, Dropout\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1DTranspose(input_tensor, filters, kernel_size, strides=2, padding='valid', output_padding=0, name=None):\n",
    "    \"\"\"\n",
    "        input_tensor: tensor, with the shape (batch_size, time_steps, dims)\n",
    "        filters: int, output dimension, i.e. the output tensor will have the shape of (batch_size, time_steps, filters)\n",
    "        kernel_size: int, size of the convolution kernel\n",
    "        strides: int, convolution step size\n",
    "        padding: 'same' | 'valid'\n",
    "    \"\"\"\n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1), padding=padding, output_padding=(output_padding,0), name=name)(x)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### settings\n",
    "# user defined span (following Yip et al and Portillo et al)\n",
    "root_models     = '/global/cscratch1/sd/vboehm/Models/SDSS_AE/'\n",
    "\n",
    "root_encoded    = '/global/cscratch1/sd/vboehm/Datasets/encoded/sdss/'\n",
    "root_decoded    = '/global/cscratch1/sd/vboehm/Datasets/decoded/sdss/'\n",
    "root_model_data = '/global/cscratch1/sd/vboehm/Datasets/sdss/by_model/'\n",
    "root_data       = '/global/cscratch1/sd/vboehm/Datasets'\n",
    "\n",
    "root_prepped    = os.path.join(root_data,'sdss/prepped')\n",
    "\n",
    "wlmin, wlmax    = (3388,8318)\n",
    "fixed_num_bins  = 1000\n",
    "\n",
    "label           = 'galaxies_quasars_bins1000_wl3388-8318'\n",
    "label_          = label+'_minz005_maxz036_minSN50'\n",
    "label_2         = label+'_minz01_maxz036_minSN50_good'+'_10_fully_connected_mean_div_conditional'\n",
    "\n",
    "seed            = 8720\n",
    "\n",
    "latent_dim      = 10\n",
    "network_type    = 'fully_connected'\n",
    "cond_on         = 'type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_range      = (np.log10(wlmin),np.log10(wlmax))\n",
    "# new binning \n",
    "new_wl        = np.logspace(wl_range[0],wl_range[1],fixed_num_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88152 0\n",
      "87988 1\n",
      "88386 2\n",
      "87950 3\n",
      "88034 4\n",
      "26440 5\n"
     ]
    }
   ],
   "source": [
    "### load data and join\n",
    "res_fluxes, res_masks, res_inv_vars = [], [] ,[]\n",
    "redshifts, SNs, ras, decs, category, sublabel = [], [], [], [], [], []\n",
    "for nn in range(0,6):\n",
    "    res_fluxes_, res_masks_, res_inv_vars_ = np.load(os.path.join(root_prepped,'prepped_data_spectra_%s_batch%d.npy'%(label_,nn)))\n",
    "    redshifts_, SNs_, ras_, decs_, category_, sublabel_, MJD_, plate_id_, fiber_ =  np.load(os.path.join(root_prepped,'prepped_data_prop_%s_batch%d.npy'%(label_,nn)))\n",
    "    if nn==0:\n",
    "        res_fluxes, res_masks, res_inv_vars, redshifts, SNs, ras, decs, category, sublabel, MJD, plate_id, fiber = res_fluxes_, res_masks_, res_inv_vars_, redshifts_, SNs_, ras_, decs_, category_, sublabel_, MJD_, plate_id_, fiber_\n",
    "    else:\n",
    "        res_fluxes   = np.concatenate([res_fluxes, res_fluxes_],axis=0)\n",
    "        res_masks    = np.concatenate([res_masks, res_masks_],axis=0)\n",
    "        res_inv_vars = np.concatenate([res_inv_vars, res_inv_vars_],axis=0)\n",
    "        redshifts    = np.concatenate([redshifts, redshifts_],axis=0)\n",
    "        SNs          = np.concatenate([SNs, SNs_],axis=0)\n",
    "        ras          = np.concatenate([ras, ras_],axis=0)\n",
    "        decs         = np.concatenate([decs, decs_],axis=0)\n",
    "        category     = np.concatenate([category, category_],axis=0)\n",
    "        sublabel     = np.concatenate([sublabel, sublabel_],axis=0)\n",
    "        MJD          = np.concatenate([MJD, MJD_],axis=0)\n",
    "        plate_id     = np.concatenate([plate_id, plate_id_],axis=0)\n",
    "        fiber        = np.concatenate([fiber, fiber_],axis=0)\n",
    "    print(len(res_fluxes_),nn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'53473.0' b'2108.0' b'50.0' b'181.9969024658203' b'37.638492584228516' b'0.10390836745500565'\n",
      "b'56902.0' b'7616.0' b'941.0' b'11.321800231933594' b'21.414342880249023' b'0.32158181071281433'\n",
      "b'55955.0' b'5406.0' b'136.0' b'189.802734375' b'9.319830894470215' b'0.08376356214284897'\n",
      "b'55452.0' b'4085.0' b'107.0' b'322.25970458984375' b'6.486550331115723' b'0.23032799363136292'\n",
      "b'54540.0' b'2786.0' b'390.0' b'213.32225036621094' b'22.519697189331055' b'0.13797487318515778'\n"
     ]
    }
   ],
   "source": [
    "for ii in range(5):\n",
    "    print(MJD[ii], plate_id[ii], fiber[ii], ras[ii], decs[ii], redshifts[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_cond_block(x,z,num, non_lin=True):\n",
    "    x = tf.concat([x,z], axis=1)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(num)(x)\n",
    "    if non_lin:\n",
    "        x = LeakyReLU()(x)\n",
    "    return Reshape((num,1))(x)\n",
    "\n",
    "def dense_block(x,num, non_lin=True):\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(num)(x)\n",
    "    if non_lin:\n",
    "        x = LeakyReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = fixed_num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0sQyEd6-dhq",
    "outputId": "8e9b7513-56a3-4bea-bbc6-34c587a4f6d4"
   },
   "outputs": [],
   "source": [
    "input        = Input(shape=(dim,1))\n",
    "input_mask   = Input(shape=(dim,1))\n",
    "input_noise  = Input(shape=(dim,1))\n",
    "input_type   = Input(shape=(1,1))\n",
    "input_params = Input(shape=(1,1))\n",
    "\n",
    "if cond_on=='type':\n",
    "    z = input_type\n",
    "if cond_on=='redshift':\n",
    "    z = input_params\n",
    "\n",
    "x = dense_block(input,128, non_lin=False)\n",
    "x = dense_block(x,64)\n",
    "x = dense_block(x,32)\n",
    "x = dense_block(x,16)\n",
    "x = dense_block(x,latent_dim,non_lin=False)\n",
    "latent = BatchNormalization(trainable=False)(x)\n",
    "x = Reshape((latent_dim,1))(latent)\n",
    "x = dense_cond_block(x,z,16)\n",
    "x = dense_cond_block(x,z,32)\n",
    "x = dense_cond_block(x,z,64)\n",
    "x = dense_cond_block(x,z,128, non_lin=False)\n",
    "x = dense_cond_block(x,z,dim, non_lin=False)\n",
    "#x = Reshape((dim,1))(x)\n",
    "\n",
    "\n",
    "\n",
    "def lossFunction(y_true,y_pred,mask,inverse):\n",
    "        loss = tf.math.square(y_true-y_pred)*inverse\n",
    "        loss = tf.reduce_mean(tf.boolean_mask(loss,mask))\n",
    "        return loss\n",
    "    \n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def compile(self, optimizer, my_loss,metrics, run_eagerly):\n",
    "        super().compile(optimizer,metrics, run_eagerly)\n",
    "        self.my_loss = my_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data_adapter.expand_1d(data)\n",
    "        input_data = data_adapter.unpack_x_y_sample_weight(data)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(data, training=True)\n",
    "            loss_value = self.my_loss(input_data[0][0],y_pred,input_data[0][1],input_data[0][2])\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return {\"training_loss\": loss_value}\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1000)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           528         flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 16)           0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           170         flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10)           40          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10, 1)        0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 11, 1)]      0           reshape[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 11)           0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           192         flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 16, 1)        0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 17, 1)]      0           reshape_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 17)           0           tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           576         flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 32, 1)        0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 33, 1)]      0           reshape_2[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 33)           0           tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           2176        flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 64, 1)        0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo [(None, 65, 1)]      0           reshape_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 65)           0           tf_op_layer_concat_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          8448        flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 128, 1)       0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_4 (TensorFlo [(None, 129, 1)]     0           reshape_4[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 129)          0           tf_op_layer_concat_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1000)         130000      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1000, 1)      0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 280,594\n",
      "Trainable params: 280,554\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "lstm_ae = CustomModel(inputs=[input,input_mask,input_noise, input_type, input_params], outputs=x)\n",
    "lstm_ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), my_loss=lossFunction, metrics=[],run_eagerly=False)\n",
    "\n",
    "# lstm_ae = Model(inputs=[input, input1,input2, input_float], outputs=[x, input1,input2, input_float])\n",
    "# lstm_ae.compile(optimizer='adam', loss=lossFunction)\n",
    "print(lstm_ae.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scheduler(length, initial_lr,factor=1.2):\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < length:\n",
    "            lr = initial_lr\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-factor)\n",
    "        \n",
    "    return scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_SIZE = len(res_fluxes)\n",
    "train_size   = int(0.7* DATASET_SIZE)\n",
    "test_size    = int(0.1 * DATASET_SIZE)\n",
    "valid_size   = int(0.2 * DATASET_SIZE)\n",
    "indices      = np.arange(DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshuffle(list_, indices):\n",
    "    res = []\n",
    "    for item_ in list_:\n",
    "        res.append(item_[indices])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fluxes, res_masks, res_inv_vars, redshifts, SNs, ras, decs, category, sublabel, MJD, plate_id, fiber = reshuffle([res_fluxes, res_masks, res_inv_vars, redshifts, SNs, ras, decs, category, sublabel,MJD, plate_id, fiber], indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new datasize:  412847\n"
     ]
    }
   ],
   "source": [
    "def redshift_bin(min_z, max_z, z_array, arrays):\n",
    "    num  = len(arrays)\n",
    "    z_array = np.asarray(z_array, dtype=np.float32)\n",
    "    indx = np.where((z_array>min_z)*(z_array<max_z))[0]\n",
    "    arrays_new = [arrays[ii][indx] for ii in range(num)]\n",
    "    print('new datasize: ', len(indx))\n",
    "    return arrays_new\n",
    "\n",
    "res_fluxes, res_masks, res_inv_vars, redshifts, SNs, ras, decs, category, sublabel, MJD, plate_id, fiber = redshift_bin(.1, .36, redshifts, [res_fluxes, res_masks, res_inv_vars, redshifts, SNs, ras, decs, category, sublabel, MJD, plate_id, fiber])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(res_fluxes)\n",
    "std  = np.std(res_fluxes)\n",
    "\n",
    "#### if only divide by mean\n",
    "std = mean\n",
    "mean= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205161.4829501925\n"
     ]
    }
   ],
   "source": [
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data   = np.expand_dims((res_fluxes[:train_size]-mean)/std,-1)\n",
    "valid_data   = np.expand_dims((res_fluxes[train_size:train_size+valid_size]-mean)/std,-1)\n",
    "test_data    = np.expand_dims((res_fluxes[-test_size::]-mean)/std,-1)\n",
    "\n",
    "train_mask   = np.expand_dims(res_masks[:train_size],-1)\n",
    "valid_mask   = np.expand_dims(res_masks[train_size:train_size+valid_size],-1)\n",
    "test_mask    = np.expand_dims(res_masks[-test_size::],-1)\n",
    "\n",
    "train_noise  = np.expand_dims(res_inv_vars[:train_size],-1)*std**2\n",
    "valid_noise  = np.expand_dims(res_inv_vars[train_size:train_size+valid_size],-1)*std**2\n",
    "test_noise   = np.expand_dims(res_inv_vars[-test_size::],-1)*std**2\n",
    "\n",
    "train_params = np.asarray(redshifts, dtype=np.float32)[:train_size]\n",
    "valid_params = np.asarray(redshifts, dtype=np.float32)[train_size:train_size+valid_size]\n",
    "test_params  = np.asarray(redshifts, dtype=np.float32)[-test_size::]\n",
    "\n",
    "train_ras = np.asarray(ras, dtype=np.float32)[:train_size]\n",
    "valid_ras = np.asarray(ras, dtype=np.float32)[train_size:train_size+valid_size]\n",
    "test_ras  = np.asarray(ras, dtype=np.float32)[-test_size::]\n",
    "\n",
    "train_decs = np.asarray(decs, dtype=np.float32)[:train_size]\n",
    "valid_decs = np.asarray(decs, dtype=np.float32)[train_size:train_size+valid_size]\n",
    "test_decs  = np.asarray(decs, dtype=np.float32)[-test_size::]\n",
    "\n",
    "train_cat = category[:train_size]\n",
    "valid_cat = category[train_size:train_size+valid_size]\n",
    "test_cat  = category[-test_size::]\n",
    "\n",
    "train_labels = sublabel[:train_size]\n",
    "valid_labels = sublabel[train_size:train_size+valid_size]\n",
    "test_labels  = sublabel[-test_size::]\n",
    "\n",
    "valid_MJD = MJD[train_size:train_size+valid_size]\n",
    "test_MJD = MJD[-test_size::]\n",
    "\n",
    "valid_plate_id = plate_id[train_size:train_size+valid_size]\n",
    "test_plate_id = plate_id[-test_size::]\n",
    "\n",
    "valid_fiber = fiber[train_size:train_size+valid_size]\n",
    "test_fiber = fiber[-test_size::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'' b'AGN' b'AGN BROADLINE' b'BROADLINE' b'STARBURST'\n",
      " b'STARBURST BROADLINE' b'STARFORMING' b'STARFORMING BROADLINE'] [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_types = le.transform(train_labels)\n",
    "valid_types = le.transform(valid_labels)\n",
    "test_types = le.transform(test_labels)\n",
    "print(le.classes_, le.transform(le.classes_))\n",
    "\n",
    "_ = plt.hist(test_types,density=True, bins=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_types  = train_types.astype('float64').reshape((-1,1,1))\n",
    "train_params = train_params.astype('float64').reshape((-1,1,1))\n",
    "valid_types  = valid_types.astype('float64').reshape((-1,1,1))\n",
    "valid_params = valid_params.astype('float64').reshape((-1,1,1))\n",
    "test_types  = test_types.astype('float64').reshape((-1,1,1))\n",
    "test_params = test_params.astype('float64').reshape((-1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(root_model_data,'train_%s.npy'%label_2),np.squeeze(train_data),np.squeeze(train_mask),np.squeeze(train_noise),train_params,train_ras, train_decs, train_cat, train_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(root_model_data,'valid_%s.npy'%label_2),np.squeeze(valid_data),np.squeeze(valid_mask),np.squeeze(valid_noise),valid_params,valid_ras, valid_decs, valid_cat, valid_types, valid_MJD, valid_plate_id, valid_fiber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(root_model_data,'test_%s.npy'%label_2),np.squeeze(test_data),np.squeeze(test_mask),np.squeeze(test_noise),test_params,test_ras, test_decs, test_cat, test_types, test_MJD, test_plate_id, test_fiber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBC_DUoQDfc4",
    "outputId": "71223b29-ac86-44df-c137-4b787686051b"
   },
   "outputs": [],
   "source": [
    "def training_cycle(BATCH_SIZE, n_epochs, lr_anneal, lr_initial, reduce_fac): \n",
    "    scheduler = make_scheduler(lr_anneal, lr_initial, reduce_fac)\n",
    "    callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    history = lstm_ae.fit(x=(train_data,train_mask,train_noise, train_types, train_params), batch_size=BATCH_SIZE, epochs=n_epochs, callbacks=[callback])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    loss = (y_true[0]-y_pred)**2*y_true[2]\n",
    "    valid_loss = np.mean(loss[np.where(y_true[1])])\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(train_data, batch_size):\n",
    "    index   = np.arange(len(train_data[0]))\n",
    "    metrics = []\n",
    "    for ii in range(10):\n",
    "        print(ii)\n",
    "        sample = np.random.choice(index, size=batch_size, replace=False, p=None)\n",
    "        res_train = lstm_ae.predict((train_data[0][sample],train_data[1][sample],train_data[2][sample], train_data[3][sample],train_data[4][sample]))\n",
    "        metric    = custom_metric((train_data[0][sample],train_data[1][sample],train_data[2][sample], train_data[3][sample],train_data[4][sample]),res_train)\n",
    "        metrics.append(metric)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10215/10215 [==============================] - 33s 3ms/step - training_loss: 3.9279 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 2.2780 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 2.0708 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.8842 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.9089 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.7637 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.7502 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "10215/10215 [==============================] - 21s 2ms/step - training_loss: 1.7647 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.7283 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.6312 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.6947 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.6182 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.6248 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5575 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "10215/10215 [==============================] - 21s 2ms/step - training_loss: 1.5609 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5517 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5597 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "10215/10215 [==============================] - 21s 2ms/step - training_loss: 1.5001 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.4968 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5309 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.4968 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5427 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.4903 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.4990 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5000 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.4833 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.4923 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.5112 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.3451 - lr: 6.0653e-04\n",
      "Epoch 30/30\n",
      "10215/10215 [==============================] - 22s 2ms/step - training_loss: 1.2884 - lr: 3.6788e-04\n",
      "1.335013335480655\n",
      "1.3041817761682923\n",
      "Epoch 1/20\n",
      "5108/5108 [==============================] - 23s 4ms/step - training_loss: 1.3494 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "5108/5108 [==============================] - 13s 2ms/step - training_loss: 1.3498 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3754 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3977 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3695 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3443 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "5108/5108 [==============================] - 13s 2ms/step - training_loss: 1.3829 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.6904 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "5108/5108 [==============================] - 13s 2ms/step - training_loss: 1.4533 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.4694 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3933 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3506 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3549 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3473 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3787 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3461 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.4421 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.3595 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.2976 - lr: 6.0653e-04\n",
      "Epoch 20/20\n",
      "5108/5108 [==============================] - 12s 2ms/step - training_loss: 1.2605 - lr: 3.6788e-04\n",
      "1.2904204355145357\n",
      "1.2782745326296552\n"
     ]
    }
   ],
   "source": [
    "histories =[]\n",
    "for batchsize, nepochs, lr_ann in zip([32,64],[30,20],[28,18]):\n",
    "    histories.append(training_cycle(batchsize, nepochs, lr_ann, 1e-3, 0.5))\n",
    "    res_valid = lstm_ae.predict((valid_data,valid_mask,valid_noise, valid_types, valid_params))\n",
    "    print(custom_metric((valid_data,valid_mask,valid_noise, valid_types, valid_params),res_valid))\n",
    "    res_train = lstm_ae.predict((train_data[:len(valid_data)],train_mask[:len(valid_data)],train_noise[:len(valid_data)],train_types[:len(valid_data)], train_params[:len(valid_data)]))\n",
    "    print(custom_metric((train_data[:len(valid_data)],train_mask[:len(valid_data)],train_noise[:len(valid_data)],train_types[:len(valid_data)], train_params[:len(valid_data)]),res_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'']\n",
      "validation:  1.175256979463938\n",
      "training:  1.1923269623546873\n",
      "[b'AGN']\n",
      "validation:  1.553184804907516\n",
      "training:  1.522500852284251\n",
      "[b'AGN BROADLINE']\n",
      "validation:  3.2773493310314814\n",
      "training:  2.942196280169367\n",
      "[b'BROADLINE']\n",
      "validation:  2.5956523604276716\n",
      "training:  2.6677419657570316\n",
      "[b'STARBURST']\n",
      "validation:  2.2804517995166678\n",
      "training:  1.6208755716436891\n",
      "[b'STARBURST BROADLINE']\n",
      "validation:  2.8313561880146327\n",
      "training:  3.0024722726375317\n",
      "[b'STARFORMING']\n",
      "validation:  1.3991452878323578\n",
      "training:  1.4029762333996765\n"
     ]
    }
   ],
   "source": [
    "for label in np.arange(7):\n",
    "    print(le.inverse_transform([label]))\n",
    "    index = np.where(valid_types==label)[0]\n",
    "    res_valid = lstm_ae.predict((valid_data[index],valid_mask[index],valid_noise[index], valid_types[index], valid_params[index]))\n",
    "    print('validation: ', custom_metric((valid_data[index],valid_mask[index],valid_noise[index], valid_types[index], valid_params[index]),res_valid))\n",
    "    index = np.where(train_types==label)[0]\n",
    "    lng   = min(len(res_valid),len(index))\n",
    "    res_train = lstm_ae.predict((train_data[index][:lng],train_mask[index][:lng],train_noise[index][:lng],train_types[index][:lng], train_params[index][:lng]))\n",
    "    print('training: ', custom_metric((train_data[index][:lng],train_mask[index][:lng],train_noise[index][:lng],train_types[index][:lng], train_params[index][:lng]),res_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[index[0],:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /global/homes/v/vboehm/.conda/envs/tf22/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /global/cscratch1/sd/vboehm/Models/SDSS_AE/full_ae_galaxies_quasars_bins1000_wl3388-8318_minz01_maxz036_minSN50_good_10_fully_connected_mean_div_conditional_decoder/assets\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(root_models,'full_ae_%s_decoder'%label_2)\n",
    "lstm_ae.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_valid = lstm_ae.predict((valid_data,valid_mask,valid_noise, valid_types, valid_params))\n",
    "custom_metric((valid_data,valid_mask,valid_noise, valid_types, valid_params),res_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap([train_data,train_mask,train_noise, train_types, train_params], batch_size=len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.append(histories[0].history['training_loss'],histories[1].history['training_loss']))\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsuE0P7RSsGN"
   },
   "outputs": [],
   "source": [
    "### concatenate and split input\n",
    "\n",
    "def extract_layers(main_model, starting_layer_ix, ending_layer_ix):\n",
    "    new_model = Sequential()\n",
    "    for ix in range(starting_layer_ix, ending_layer_ix):\n",
    "        curr_layer = main_model.get_layer(index=ix)\n",
    "        print(ix, curr_layer)\n",
    "        new_model.add(curr_layer)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ae.get_layer(index=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = extract_layers(lstm_ae,2,10)\n",
    "\n",
    "#decoder = extract_layers(lstm_ae,10,22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.build((None,fixed_num_bins,1))\n",
    "#decoder.build((None,latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_valid = encoder.predict(valid_data)\n",
    "encoded_train = encoder.predict(train_data)\n",
    "encoded_test = encoder.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_valid = decoder.predict(encoded_valid)\n",
    "decoded_train = decoder.predict(encoded_train)\n",
    "decoded_test = decoder.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_models,'encoder_%s'%label_2)\n",
    "encoder.save(path)\n",
    "path = os.path.join(root_models,'decoder_%s'%label_2)\n",
    "decoder.save(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_models,'encoder_%s'%label_2)\n",
    "encoder = tf.keras.models.load_model(path)\n",
    "path = os.path.join(root_models,'decoder_%s'%label_2)\n",
    "decoder = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(root_encoded,'encoded_%s.npy'%label_2),[encoded_train, encoded_valid, encoded_test])\n",
    "encoded_train, encoded_valid, encoded_test = np.load(os.path.join(root_encoded,'encoded_%s.npy'%label_2), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(root_decoded,'decoded_%s.npy'%label_2),[decoded_train,decoded_valid, decoded_test, mean])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(encoded_valid.flatten(),bins=1000,density=True, log=True)\n",
    "_=plt.hist(encoded_train[:len(encoded_valid)].flatten(),bins=1000,density=True, alpha=0.5, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = 50\n",
    "\n",
    "fig, ax = plt.subplots(2,latent_dim//2, figsize=(20,4))\n",
    "ax=ax.flatten()\n",
    "for ii in range(latent_dim):\n",
    "    jj= (ii+1)%latent_dim\n",
    "    im = ax[ii].scatter(encoded_valid[:,ii],encoded_valid[:,jj],c=valid_params, cmap='nipy_spectral',s=1)\n",
    "    plt.colorbar(im, ax=ax[ii])\n",
    "    ax[ii].set_xlim(-lims,lims)\n",
    "    ax[ii].set_ylim(-lims,lims)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2,latent_dim//2, figsize=(20,4))\n",
    "ax=ax.flatten()\n",
    "for ii in range(latent_dim):\n",
    "    jj= (ii+1)%latent_dim\n",
    "    im = ax[ii].scatter(encoded_train[:47000,ii],encoded_train[:47000,jj],c=train_params[:47000], cmap='nipy_spectral',s=1)\n",
    "    plt.colorbar(im, ax=ax[ii])\n",
    "    ax[ii].set_xlim(-lims,lims)\n",
    "    ax[ii].set_ylim(-lims,lims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sublabel)\n",
    "print(le.classes_, le.transform(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = 50\n",
    "\n",
    "fig, ax = plt.subplots(2,latent_dim//2, figsize=(20,4))\n",
    "ax=ax.flatten()\n",
    "for ii in range(latent_dim):\n",
    "    jj= (ii+1)%latent_dim\n",
    "    im = ax[ii].scatter(encoded_valid[:,ii],encoded_valid[:,jj],c=le.transform(valid_labels), cmap='Set1', s=1)\n",
    "    plt.colorbar(im, ax=ax[ii])\n",
    "    ax[ii].set_xlim(-lims,lims)\n",
    "    ax[ii].set_ylim(-lims,lims)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2,latent_dim//2, figsize=(20,4))\n",
    "ax=ax.flatten()\n",
    "for ii in range(latent_dim):\n",
    "    jj= (ii+1)%latent_dim\n",
    "    im = ax[ii].scatter(encoded_train[:47000,ii],encoded_train[:47000,jj],c=le.transform(train_labels)[:47000], cmap='Set1', s=1)\n",
    "    plt.colorbar(im, ax=ax[ii])\n",
    "    ax[ii].set_xlim(-lims,lims)\n",
    "    ax[ii].set_ylim(-lims,lims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_list=['dark_background']\n",
    "# Plot a demonstration figure for every available style sheet.\n",
    "for style_label in style_list:\n",
    "    with plt.rc_context({\"figure.max_open_warning\": len(style_list)}):\n",
    "        with plt.style.context(style_label):\n",
    "            fig, ax = plt.subplots(5,5, figsize=(25,15))\n",
    "            ax = ax.flatten()\n",
    "            for nn, ii in enumerate(np.arange(25)):\n",
    "                ax[nn].plot(new_wl[:-1], (np.squeeze(valid_data*std)[ii]+mean)*np.squeeze(valid_mask)[ii], )\n",
    "                ax[nn].plot(new_wl[:-1], (np.squeeze(decoded_valid*std)[ii]+mean)*np.squeeze(valid_mask)[ii], color='orange')\n",
    "                ax[nn].plot(new_wl[:-1], (np.squeeze(decoded_valid*std)[ii]+mean), color='red')\n",
    "                ax[nn].text(0.05, 0.92, r'z=%.2f'%valid_params[ii], fontsize=15, color='white',verticalalignment='top', horizontalalignment='left', transform=ax[nn].transAxes)\n",
    "                ax[nn].text(0.05, 0.82, r'label=%s,%s'%(valid_labels[ii],valid_cat[ii]), fontsize=15, color='white',verticalalignment='top', horizontalalignment='left', transform=ax[nn].transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens=[]\n",
    "for ii in range(latent_dim):\n",
    "    res = np.mean(enc,axis=0)\n",
    "    res[ii]+=1\n",
    "    sens+=[res]\n",
    "for ii in range(latent_dim):\n",
    "    res = np.mean(enc,axis=0)\n",
    "    res[ii]-=1\n",
    "    sens+=[res]\n",
    "sens+=[np.mean(enc,axis=0)]\n",
    "sens = np.asarray(sens)\n",
    "test = decoder.predict(sens)\n",
    "\n",
    "for ii in range(8):\n",
    "    plt.plot(test[ii]-test[8+ii])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNbeXyhiihZQbSo0juhrHje",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LSTM-AE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf22",
   "language": "python",
   "name": "tf22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
